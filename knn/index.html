



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Duties of Information System">
      
      
        <link rel="canonical" href="https://listiynrn.github.io/170441100054/knn/">
      
      
        <meta name="author" content="Listiyo Nuraini">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>K-NEAREST NEIGHBOR CLASSIFIER - INFORMATION SYSTEM</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#a-penjelasan-k-nearest-neighbor-knn" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://listiynrn.github.io/170441100054/" title="INFORMATION SYSTEM" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              INFORMATION SYSTEM
            </span>
            <span class="md-header-nav__topic">
              K-NEAREST NEIGHBOR CLASSIFIER
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/listiynrn/170441100054" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    listiynrn/170441100054
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="DATA MINING" class="md-tabs__link md-tabs__link--active">
        DATA MINING
      </a>
    
  </li>

      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://listiynrn.github.io/170441100054/" title="INFORMATION SYSTEM" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    DATA MINING
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/listiynrn/170441100054" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    listiynrn/170441100054
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="PENGANTAR" class="md-nav__link">
      PENGANTAR
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
    <a href="./" title="K-NEAREST NEIGHBOR CLASSIFIER" class="md-nav__link md-nav__link--active">
      K-NEAREST NEIGHBOR CLASSIFIER
    </a>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../decision-tree/" title="DECISION TREE CLASSIFIER" class="md-nav__link">
      DECISION TREE CLASSIFIER
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#PENJELASAN-K-NEAREST-NEIGHBOR" title="A. PENJELASAN K-NEAREST NEIGHBOR" class="md-nav__link">
   A. PENJELASAN K-NEAREST NEIGHBOR
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ALGORITMA-PERHITUNGAN-KNN" title="B. ALGORITMA PERHITUNGAN KNN" class="md-nav__link">
    B. ALGORITMA PERHITUNGAN KNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#KELEBIHAN-DAN-KEKURANGAN-KNN" title="C. KELEBIHAN DAN KEKURANGAN KNN" class="md-nav__link">
    C. KELEBIHAN DAN KEKURANGAN KNN
  </a>
  
</li>
      
      
        <li class="md-nav__item">
  <a href="#IMPLEMENTASI-HABERMAN-DATASET-DENGAN-KNN-MENGGUNAKAN-PYTHON-SCIKIT-LEARN" title="D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN" class="md-nav__link">
    D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN
  </a>
  
</li>
   
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
       
        <li class="md-nav__item">
  <a href="#PENJELASAN-K-NEAREST-NEIGHBOR" title="A. PENJELASAN K-NEAREST NEIGHBOR" class="md-nav__link">
   A. PENJELASAN K-NEAREST NEIGHBOR
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ALGORITMA-PERHITUNGAN-KNN" title="B. ALGORITMA PERHITUNGAN KNN" class="md-nav__link">
    B. ALGORITMA PERHITUNGAN KNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#KELEBIHAN-DAN-KEKURANGAN-KNN" title="C. KELEBIHAN DAN KEKURANGAN KNN" class="md-nav__link">
    C. KELEBIHAN DAN KEKURANGAN KNN
  </a>
  
</li>
      
      
        <li class="md-nav__item">
  <a href="#IMPLEMENTASI-HABERMAN-DATASET-DENGAN-KNN-MENGGUNAKAN-PYTHON-SCIKIT-LEARN" title="D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN" class="md-nav__link">
    D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN
  </a>
  
</li>
      
      
    </ul>
  

    
   
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="a-penjelasan-k-nearest-neighbor-knn"><strong>A. PENJELASAN K-NEAREST NEIGHBOR</strong> <small>(KNN)</small><a class="headerlink" href="#a-penjelasan-k-nearest-neighbor-knn" title="Permanent link">&para;</a></h1>
<hr />
<p>Algoritme <em>k-nearest neighbor</em> (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. </p>
<p>K-nearest neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori K-tetangga terdekat. </p>
<p>KNN digunakan dalam banyak aplikasi data mining, statistical pattern recognition, image processing, dll. Beberapa aplikasinya meliputi: pengenalan tulisan tangan, satellite image dan ECG pattern. ECG produces apatternreflecting the electrical activity of the heart.</p>
<p>Data pembelajaran diproyeksikan ke ruang berdimensi banyak,  dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang  ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data  pembelajaran. Sebuah titik pada ruang ini ditandai kelas <em>c</em> jika kelas <em>c</em> merupakan klasifikasi yang paling banyak ditemui pada <em>k</em> buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. </p>
<p>Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan  vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase  klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang  klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini  terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah <em>k</em>  buah yang paling dekat diambil. Titik yang baru klasifikasinya  diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik  tersebut. </p>
<p>Nilai <em>k</em> yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai <em>k</em> yang tinggi akan mengurangi efek <em>noise</em> pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai <em>k</em>  yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan  menggunakan cross-validation. Kasus khusus di mana klasifikasi  diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan  kata lain, <em>k</em> = 1) disebut algoritme <em>nearest neighbor</em>. </p>
<p>Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau  tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut  tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap  algoritme ini sebagian besar membahas bagaimana memilih dan memberi  bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. </p>
<p>Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: </p>
<ul>
<li>Linear scan</li>
<li>Pohon kd</li>
<li>Pohon Balltree</li>
<li>Pohon metrik</li>
<li>Locally-sensitive hashing (LSH)</li>
</ul>
<p>Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin <em>error rate</em> yang tidak lebih dari dua kali <em>Bayes error rate</em> (<em>error rate</em> minimum untuk distribusi data tertentu).</p>
<h1 id="b-algoritma-perhitungan-knn"><strong>B. ALGORITMA PERHITUNGAN KNN</strong><a class="headerlink" href="#b-algoritma-perhitungan-knn" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="langkah-langkah-untuk-menghitung-knn"><strong>Langkah-Langkah untuk menghitung KNN</strong><a class="headerlink" href="#langkah-langkah-untuk-menghitung-knn" title="Permanent link">&para;</a></h2>
<ol>
<li>Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru.</li>
<li>Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training.</li>
<li>Urutkan hasil perhitungan tersebut.</li>
<li>Tentukan tetangga terdekat berdasarkan jarak minimum ke K.</li>
<li>Tentukan kategori dari tetangga terdekat dengan objek/data.</li>
<li>Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.</li>
</ol>
<h2 id="perhitungan-numerik-knn-perhitungan-dengan-cara-manual"><strong>Perhitungan Numerik KNN <small>(Perhitungan dengan cara manual)</strong></small><a class="headerlink" href="#perhitungan-numerik-knn-perhitungan-dengan-cara-manual" title="Permanent link">&para;</a></h2>
<hr />
<p>dalam menghitung jarak ada 4 cara tergantung yang makai;</p>
<ol>
<li>
<p><strong>Euclidean Distance</strong>
   $$
   \begin{equation}
   d(x, y)=\sqrt{\sum_{i=1}<sup>{m}\left(x_{i}-y_{i}\right)</sup>{2}}
   \end{equation}
   $$
   ide rumus ini dari pytaghoras 
   $$
   \begin{equation}
   c=\sqrt{a<sup>{2}+b</sup>{2}}
   \end{equation}
   $$
   d(x,y) dibaca <em>distance</em> antara x dan y</p>
</li>
<li>
<p><strong>Manhattan Distance</strong>
   $$
   \begin{equation}
   d(x, y)=\sum_{i=1}^{m}\left|x_{i}-y_{i}\right|
   \end{equation}
   $$
   *rumus ini mencari jarak hanya dengan <strong>menjumlahkan</strong> semua <strong>selisih</strong> dari <strong>jarak</strong> <img alt="x_i" src="https://belajarkalkulus.com/wp-content/ql-cache/quicklatex.com-c8700e0258243116de0d4f288e2e3b44_l3.png" /> dan <img alt="y_i" src="https://belajarkalkulus.com/wp-content/ql-cache/quicklatex.com-bb3c186e5c65fcd066bb23dec8f4e48a_l3.png" />.</p>
</li>
</ol>
<p><em>Mungkin</em> idenya dari menghitung jarak dari 3 ke 5 yaitu 2 karena |3-5|=2.</p>
<p>Apa bedanya dengan <em>euclidean distance</em>? manhattan distance itu :</p>
<p>​    Mengurangi per elemen antar 2 variabel, memutlakannya lalu menjumlahkannya. Sedangkan euclidean distance menghitung jarak antara 2 titik dengan konsep pythagoras.</p>
<ol>
<li>
<p><strong>Minkowsky Distance</strong>
   $$
   \begin{equation}
   d(x, y)=\left(\sum_{i=1}<sup>{m}\left|x_{i}-y_{i}\right|</sup>{r}\right)^{1 / r}
   \end{equation}
   $$
   ide rumus ini diambil dari konsep aljabar dengan objek vektor berdimensi n dan r bukan 1 dan 2. <em>why?</em> Karena kalau <img alt="r=1" src="https://belajarkalkulus.com/wp-content/ql-cache/quicklatex.com-020a271427b6fdedce143bb26dac3c27_l3.png" /> maka akan terbntuk manhattan distance, kalau <img alt="r=2" src="https://belajarkalkulus.com/wp-content/ql-cache/quicklatex.com-0562ab0fec4ce668dc1c02b55f6b3880_l3.png" /> euclidean distance.</p>
</li>
<li>
<p><strong>Chebychev Distance</strong>
   $$
   \begin{equation}
   d(x, y)=\max <em>{i=1}\left|x</em>{i}-y_{i}\right|
   \end{equation}
   $$
   *rumus ini mencari jarak yang terbesar antara x_i dan y_i</p>
</li>
</ol>
<p><em>Fact</em>: Algoritma ini adalah algoritma yang paling simpel dari semua algoritma <em>machine learning.</em></p>
<p><em>Fun</em> : Algoritma ini bisa dipakai untuk “mesin pencari” seperti google.</p>
<h3 id="contoh"><strong><u>Contoh</u></strong><a class="headerlink" href="#contoh" title="Permanent link">&para;</a></h3>
<p>didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya:</p>
<table>
<thead>
<tr>
<th>X1=Daya Tahan Kesamaan (detik)</th>
<th>X2=Kekuatan (Kg/meter persegi)</th>
<th align="center">Klasifikasi</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>7</td>
<td align="center">Jelek</td>
</tr>
<tr>
<td>7</td>
<td>4</td>
<td align="center">Jelek</td>
</tr>
<tr>
<td>3</td>
<td>4</td>
<td align="center">Bagus</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td align="center">Bagus</td>
</tr>
</tbody>
</table>
<p>Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini.</p>
<h3 id="penyelesaian"><u><strong>Penyelesaian</u></strong><a class="headerlink" href="#penyelesaian" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>Tentukan parameter K = jumlah banyaknya tetangga terdekat. <strong>Misal K=3</strong></p>
</li>
<li>
<p>Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training</p>
</li>
</ol>
<table>
<thead>
<tr>
<th align="center">X1=Daya Tahan Kesamaan (detik)</th>
<th align="center">X2=Kekuatan (Kg/meter persegi)</th>
<th align="center">Square Distance ke Data baru (3,7)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">7</td>
<td align="center">7</td>
<td align="center"><img alt="" src="../assets/images/distance1.PNG" /></td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance2.PNG" /></td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance3.PNG" /></td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance4.PNG" /></td>
</tr>
</tbody>
</table>
<p>Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. kami menggunakan ecludian distance</p>
<table>
<thead>
<tr>
<th align="center">X1=Daya Tahan Kesamaan (detik)</th>
<th align="center">X2=Kekuatan (Kg/meter persegi)</th>
<th align="center">Square Distance ke Data baru (3,7)</th>
<th align="center">Urutan (Ranking) Jarak</th>
<th align="center">Apakah termasuk K-NN?</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">7</td>
<td align="center">7</td>
<td align="center"><img alt="" src="../assets/images/distance1.PNG" /></td>
<td align="center">3</td>
<td align="center">YA</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance2.PNG" /></td>
<td align="center">4</td>
<td align="center">TIDAK</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance3.PNG" /></td>
<td align="center">1</td>
<td align="center">YA</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">4</td>
<td align="center"><img alt="" src="../assets/images/distance4.PNG" /></td>
<td align="center">2</td>
<td align="center">YA</td>
</tr>
</tbody>
</table>
<ol>
<li>Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K).</li>
</ol>
<table>
<thead>
<tr>
<th align="center">X1=Daya Tahan Kesamaan (detik)</th>
<th align="center">X2=Kekuatan (Kg/meter persegi)</th>
<th align="center">Square Distance ke Data baru (3,7)</th>
<th align="center">Urutan (Ranking) Jarak</th>
<th align="center">Apakah termasuk K-NN?</th>
<th align="center">Y=Category of Nearest Neighbor</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">7</td>
<td align="center">7</td>
<td align="center"><img alt="img" src="../assets/images/distance1.PNG?lastModify=1558344153" /></td>
<td align="center">3</td>
<td align="center">YA</td>
<td align="center">Jelek</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">4</td>
<td align="center"><img alt="img" src="../assets/images/distance2.PNG?lastModify=1558344153" /></td>
<td align="center">4</td>
<td align="center">TIDAK</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">4</td>
<td align="center"><img alt="img" src="../assets/images/distance3.PNG?lastModify=1558344153" /></td>
<td align="center">1</td>
<td align="center">YA</td>
<td align="center">Bagus</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">4</td>
<td align="center"><img alt="img" src="../assets/images/distance4.PNG?lastModify=1558344153" /></td>
<td align="center">2</td>
<td align="center">YA</td>
<td align="center">Bagus</td>
</tr>
</tbody>
</table>
<ol>
<li>Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru.</li>
</ol>
<p>–Kita punya2 kategori Bagus dan1 kategori Jelek, karena2&gt;1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori <strong>Bagus</strong>.</p>
<h1 id="c-kelebihan-dan-kekurangan-knn"><strong>C. KELEBIHAN DAN KEKURANGAN KNN</strong><a class="headerlink" href="#c-kelebihan-dan-kekurangan-knn" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="kelebihan"><strong>Kelebihan</strong><a class="headerlink" href="#kelebihan" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Sangat nonlinear</strong></li>
</ul>
<p>​ kNN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat nonparametrik. Pembahasan mengenai <strong>model parametrik</strong> dan model <strong>nonparametrik</strong>  bisa menjadi artikel sendiri, namun secara singkat, definisi model  nonparametrik adalah model yang tidak mengasumsikan apa-apa mengenai  distribusi instance di dalam dataset. Model nonparametrik biasanya lebih  sulit diinterpretasikan, namun salah satu kelebihannya adalah garis  keputusan kelas yang dihasilkan model tersebut bisa jadi sangat  fleksibel dan nonlinear.</p>
<ul>
<li><strong>Mudah dipahami dan diimplementasikan</strong></li>
</ul>
<p>​ Dari  paparan yang diberikan dan penjelasan cara menghitung jarak dalam  artikel ini, cukup jelas bahwa algoritma kNN mudah dipahami dan juga  mudah dimplementasikan. Untuk mengklasifikasi instance x menggunakan  kNN, kita cukup mendefinisikan fungsi untuk menghitung jarak  antar-instance, menghitung jarak x dengan semua instance lainnya  berdasarkan fungsi tersebut, dan menentukan kelas x sebagai kelas yang  paling banyak muncul dalam k instance terdekat.</p>
<h2 id="kekurangan"><strong>Kekurangan</strong><a class="headerlink" href="#kekurangan" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p><strong>Perlu menunjukkan parameter K (jumlah tetangga terdekat)</strong></p>
</li>
<li>
<p><strong>Tidak menangani nilai hilang (missing value) secara implisit</strong></p>
</li>
</ul>
<p>​ Jika  terdapat nilai hilang pada satu atau lebih variabel dari suatu  instance, perhitungan jarak instance tersebut dengan instance lainnya  menjadi tidak terdefinisi. Bagaimana coba, menghitung jarak dalam ruang  3-dimensi jika salah satu dimensi hilang? Karenanya, sebelum menerapkan  kNN kerap dilakukan <strong>imputasi</strong> untuk mengisi nilai-nilai  hilang yang ada pada dataset. Contoh teknik imputasi yang paling umum  adalah mengisi nilai hilang pada suatu variabel dengan nilai rata-rata  variabel tersebut (mean imputation).</p>
<ul>
<li><strong>Sensitif terhadap data pencilan (outlier)</strong></li>
</ul>
<p>​ Seperti  yang telah dijelaskan Ali pada artikel sebelumnya, kNN bisa jadi sangat  fleksibel jika k kecil. Fleksibilitas ini mengakibatkan kNN cenderung  sensitif terhadap data pencilan, khususnya pencilan yang terletak di  “tengah-tengah” kelas yang berbeda. Lebih jelasnya, perhatikan ilustrasi  di bawah. Pada gambar kiri, seluruh instance bisa diklasifikasikan  dengan benar ke dalam kelas biru dan jingga. Tetapi, ketika ditambahkan  instance biru di antara instance jingga, beberapa instance jingga  menjadi salah terklasifikasi.Perlu dipilih k yang tepat untuk mengurangi  dampak data pencilan dalam kNN.</p>
<ul>
<li><strong>Rentan terhadap variabel yang non-informatif</strong></li>
</ul>
<p>​ Meskipun  kita telah menstandardisasi rentang variabel, kNN tetap tidak dapat  mengetahui variabel mana yang signifikan dalam klasifikasi dan mana yang  tidak</p>
<ul>
<li><strong>Rentan terhadap dimensionalitas yang tinggi</strong></li>
</ul>
<p>​ Berbagai  permasalahan yang timbul dari tingginya dimensionalitas (baca:  banyaknya variabel) menimpa sebagian besar algoritma pembelajaran mesin,  dan kNN adalah salah satu algoritma yang paling rentan terhadap  tingginya dimensionalitas. Hal ini karena semakin banyak dimensi, ruang  yang bisa ditempati instance semakin besar, sehingga semakin besar pula  kemungkinan bahwa nearest neighbour dari suatu instance sebetulnya sama  sekali tidak “near“.</p>
<ul>
<li><strong>Rentan terhadap perbedaan rentang variabel</strong></li>
</ul>
<p>​ Dalam  perhitungan jarak antar-instance, kNN menganggap semua variabel setara  atau sama penting (lihat bagian penjumlahan pada rumus perhitungan jarak  di atas). Jika terdapat variabel p yang memiliki rentang jauh lebih  besar dibanding variabel-variabel lainnya, maka perhitungan jarak akan  didominasi oleh p. Misalkan ada dua variabel, a dan b, dengan rentang  variabel a 0 sampai 1.000 dan rentang variabel b 0 sampai 10. Kuadrat  selisih dua nilai variabel b tidak akan lebih dari 100, sedangkan untuk  variabel a kuadrat selisihnya bisa mencapai 1.000.000. Hal ini bisa  mengecoh kNN sehingga kNN menganggap a tidak membawa pengaruh dalam  perhitungan jarak karena rentangnya sangat besar dibanding rentang  b.Ilustrasinya diberikan di bawah ini. Manakah yang merupakan nearest  neighbour dari instance x? Jika dilihat dari “kacamata” komputer,  nearest neighbour x bukanlah y, melainkan z, Mengapa?
   Untuk  mengatasi perbedaan rentang, biasanya dilakukan preproses berupa  standardisasi rentang semua variabel sebelum menerapkan algoritma kNN</p>
<ul>
<li><strong>Nilai komputasi yang tinggi.</strong></li>
</ul>
<p>​ Untuk  mengklasifikasi sebuah instance x, kNN harus menghitung jarak antara x  dengan semua instance lain dalam dataset yang kita miliki. Dengan kata  lain, kompleksitas waktu klasifikasi kNN berbanding lurus dengan jumlah  instance latih. Jika dataset yang kita miliki berukuran besar (terdiri  dari banyak instance dan/atau banyak variabel), proses ini bisa jadi  sangat lambat. Bayangkan, jika kita punya 10.000 instance dengan  masing-masing 20 variabel dan kita ingin mengklasifikasi 100 instance  baru (instance uji), maka total operasi yang harus dilakukan menjadi:<strong>(100 instance uji x 10.000 instance latih) x 20 variabel/instance x 2 operasi/variabel = 40 juta operasi</strong> Beberapa cara pengindexan (K-D tree) dapat digunakan untuk mereduksi biaya komputasi.</p>
<h1 id="d-implementasi-haberman-dataset-dengan-knn-menggunakan-python-scikit-learn"><strong>D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN</strong><a class="headerlink" href="#d-implementasi-haberman-dataset-dengan-knn-menggunakan-python-scikit-learn" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="dataset"><strong>Dataset</strong><a class="headerlink" href="#dataset" title="Permanent link">&para;</a></h2>
<ol>
<li>Judul: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data">Haberman's Survival Data</a></li>
<li>Sources:
   (a) Donor:   Tjen-Sien Lim (<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#105;&#109;&#116;&#64;&#115;&#116;&#97;&#116;&#46;&#119;&#105;&#115;&#99;&#46;&#101;&#100;&#117;">&#108;&#105;&#109;&#116;&#64;&#115;&#116;&#97;&#116;&#46;&#119;&#105;&#115;&#99;&#46;&#101;&#100;&#117;</a>)
   (b) Date:    March 4, 1999</li>
<li>Jumlah Instances: 306</li>
<li>Jumlah Attributes: 4 (termasuk attribute class)</li>
<li>Informasi Attribute:</li>
<li>Age of patient at time of operation (numerical)</li>
<li>Patient's year of operation (year - 1900, numerical)</li>
<li>Number of positive axillary nodes detected (numerical)</li>
<li>Survival status (class attribute)
      1 = the patient survived 5 years or longer
      2 = the patient died within 5 year</li>
</ol>
<h2 id="implementasi"><strong>Implementasi</strong><a class="headerlink" href="#implementasi" title="Permanent link">&para;</a></h2>
<hr />
<h3 id="mengimport-librari"><strong>Mengimport Librari</strong><a class="headerlink" href="#mengimport-librari" title="Permanent link">&para;</a></h3>
<p>Install terlebih dahulu librari-librari yang dibutuhkan. maka, module dapat di eksekusi</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>  
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>  
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>  
</pre></div>

<h3 id="langkah-1-mengimport-dataset"><strong>Langkah 1: Mengimport DataSet</strong><a class="headerlink" href="#langkah-1-mengimport-dataset" title="Permanent link">&para;</a></h3>
<p>Untuk mengimport dan load data, anda dapat menggunkaan sintax dibawah ini:</p>
<div class="codehilite"><pre><span></span><span class="c1"># memasukkan url dataset kedalam variabel, sehingga anda tidak perlu mendownload data. cukup memanggilnya</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data&quot;</span>

<span class="c1"># memasukkan nama kolom ke dalam  dataset</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;patient-year-operation&#39;</span><span class="p">,</span> <span class="s1">&#39;nodes-detected&#39;</span><span class="p">,</span> <span class="s1">&#39;(Survival-status)Class&#39;</span><span class="p">]</span>

<span class="c1"># membaca dataset ke pandas dataframe</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</pre></div>

<p>Untuk melihat data, eksekusi code berikut:</p>
<div class="codehilite"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="c1"># atau dataset.head()</span>
</pre></div>

<p>Maka, akan menampilkan data seperti dibawah ini (row paling atas dan bawah):</p>
<p><img alt="row data teratas" src="../assets/images/open1.PNG" /></p>
<p><img alt="row data terbawah" src="../assets/images/open2.PNG" /></p>
<h3 id="langkah-2-preprocessing"><strong>Langkah 2: Preprocessing</strong><a class="headerlink" href="#langkah-2-preprocessing" title="Permanent link">&para;</a></h3>
<p>Langkah selanjutnya adalah memisahkan dataset kedalam atribut dan label. gunakan code berikut ini:</p>
<div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  
</pre></div>

<p>variabel X untuk mendefinisikan 3 Kolom pada dataset sedangkan variabel y untuk mendefinisikan label pada dataset.</p>
<h3 id="langkah-3-train-test-split"><strong>Langkah 3: Train Test Split</strong><a class="headerlink" href="#langkah-3-train-test-split" title="Permanent link">&para;</a></h3>
<p>Untuk memisahkan data menjadi data training dan data testing. performa algoritma yang saya gunakan selama fase testing tidak terlihat (un-seen data).</p>
<p>berikut code untuk membuat training dan testing data:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>  
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>  
</pre></div>

<p>script diatas memisahkan dataset 80% data training dan 20% data testing. dari 306 data, terbagi data training sebesar 245 data dan data testing sebesar 61 data.</p>
<h3 id="fitur-scaling"><strong>Fitur Scaling</strong><a class="headerlink" href="#fitur-scaling" title="Permanent link">&para;</a></h3>
<p>Sebelum kita memprediksi data yang sebenarnya. alangkah baiknya kita menggunakan fitur scaling untuk menormalisasikan data sebelum di testing.</p>
<p>berikut codenya:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>  
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>  
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  
</pre></div>

<h3 id="langkah-4-training-dan-prediction"><strong>Langkah 4: Training dan Prediction</strong><a class="headerlink" href="#langkah-4-training-dan-prediction" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>  
<span class="n">classifier</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>  
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  
</pre></div>

<p>langkah pertama adlah mengimport librari <code>KNeighborsClassifier</code> class from the <code>sklearn.neighbors</code> . Pada baris kedua, class di inisialisasi dengan parameter, i.e. <code>n_neigbours</code>.
nilai K yang diuji adalah K-7.</p>
<p>Kemudian kita memprediksi nilai dari datset:</p>
<div class="codehilite"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  
</pre></div>

<h3 id="langkah-5-evaluating-dari-algoritma"><strong>Langkah 5: Evaluating dari Algoritma</strong><a class="headerlink" href="#langkah-5-evaluating-dari-algoritma" title="Permanent link">&para;</a></h3>
<p>Untuk mengevaluasi sebuah algoritma, matrix, presisi, recall dan f1 score pada umumnya menggunnakan matrix dengan <code>confusion_matrix</code> dan <code>classification_report</code> methods dari <code>sklearn.metrics</code></p>
<p>Berikut code programnya:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>  
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>  
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>  
</pre></div>

<p>Tampilan output dari script diatas:</p>
<p><img alt="akurasi" src="../assets/images/prediksi.PNG" /></p>
<p>dari hasil output diatas dapat diketahui bahwa nilai K-7 menghasilkan 71%</p>
<h3 id="langkah-6-mengekstrak-rating-error-pada-k-value"><strong>Langkah 6: Mengekstrak rating error pada K Value</strong><a class="headerlink" href="#langkah-6-mengekstrak-rating-error-pada-k-value" title="Permanent link">&para;</a></h3>
<p>Pada testing dan evaluasi data sebelumnya. kita telah mengecek akurasi pada K-. Namun, untuk menghasilkan nilai tingkat akurasi yang tinggi kita dapat mengecek beberapa sebagai acuan tanpa mengulang-ngulang data program yaitu dengan cara plot grafik dan coresponden nilai errror pada K.</p>
<p>Untuk menghitung, nilai value saya menggunakan distance K11:40. anda dapat menggunakan script berikut:</p>
<div class="codehilite"><pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># menghitung nilai error pada K antara 1 dan 40</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">):</span>  
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred_i</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_i</span> <span class="o">!=</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

<p>Script diatas mengeksekusi perulangan 1 sampai 40. pada setiap iterasi, prediksi nilai error yang telah dihitung akan masuk dalam list error.</p>
<p>langkah selanjutnya adalah plot nilai error K. berikut script code:</p>
<div class="codehilite"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">error</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span>  
         <span class="n">markerfacecolor</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Rating Error Nilai K&#39;</span><span class="p">)</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Nilai K&#39;</span><span class="p">)</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Error&#39;</span><span class="p">)</span>  
</pre></div>

<p><strong>Output</strong></p>
<h1 id="referensi"><strong>REFERENSI</strong><a class="headerlink" href="#referensi" title="Permanent link">&para;</a></h1>
<p><a href="https://id.wikipedia.org/wiki/KNN">https://id.wikipedia.org/wiki/KNN</a> </p>
<p><a href="https://en.wikipedia.org/wiki/Feature_scaling">https://en.wikipedia.org/wiki/Feature_scaling</a></p>
<p><a href="https://belajarkalkulus.com/clustering-part-iii/">https://belajarkalkulus.com/clustering-part-iii/</a></p>
<p><a href="http://depandienda.it.student.pens.ac.id/file/knn_references.pdf">http://depandienda.it.student.pens.ac.id/file/knn_references.pdf</a> </p>
<p><a href="https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/">https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/</a> </p>
<p><a href="https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/">https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/</a></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="DATA MINING" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                DATA MINING
              </span>
            </div>
          </a>
        
        
          <a href="../decision-tree/" title="DECISION TREE CLASSIFIER" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                DECISION TREE CLASSIFIER
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 Listiyo Nuraini
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://linktree.com/spirulinaherb.co" class="md-footer-social__link fa fa-global"></a>
    
      <a href="https://github.com/listiynrn" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/listiyo_nuraini" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="http://instagram.com/listiy.nrn" class="md-footer-social__link fa fa-instagram"></a>
    
      <a href="https://linkedin.com/in/listiyo_nuraini" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>