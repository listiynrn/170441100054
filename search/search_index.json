{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PENGANTAR \u00b6 Puji syukur kehadirat ALLAH SWT dan ucapan terima kasih kepada Bapak Mulaab, S.Si., M.Kom. selaku Dosen Pengampu kami: Nama :Listiyo Nuraini NIM :170441100054 Prodi :Sistem Informasi Jurusan :Teknik Informatika","title":"DATA MINING"},{"location":"#pengantar","text":"Puji syukur kehadirat ALLAH SWT dan ucapan terima kasih kepada Bapak Mulaab, S.Si., M.Kom. selaku Dosen Pengampu kami: Nama :Listiyo Nuraini NIM :170441100054 Prodi :Sistem Informasi Jurusan :Teknik Informatika","title":"PENGANTAR"},{"location":"decision-tree/","text":"A. PENJELASAN DECISION TREE \u00b6 Decision tree atau pohon keputusan adalah alat pendukung keputusan yang menggunakan model keputusan yang berbentuk seperti pohon. Decision tree memetakan berbagai alternatif yang mungkin untuk mengatasi suatu masalah, dan terdapat juga faktor-faktor kemungkinan yang dapat mempengaruhi alternatif tersebut beserta estimasi akhirnya jika memilih alternatif yang ada. Decision tree merupakan salah satu metode yang bisa digunaan untuk menampilkan algoritma dimana hanya berisi pernyataan kontrol bersyarat. Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas\u2013kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu Decision tree terdiri dari tiga jenis simpul: \u00b6 Simpul keputusan - biasanya diwakili oleh kotak Simpul peluang - biasanya diwakili oleh lingkaran Simpul akhir - biasanya diwakili oleh segitiga Konsep Decision Tree \u00b6 Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu. Karakteristik Decision Tree \u00b6 Berikut ini adalah beberapa karakteristik decision tree secara umum : \u2022 Decision tree merupakan suatu pendekatan nonparametrik untuk membangun model klasifikasi \u2022 Teknik yang dikembangkan dalam membangun decision tree memungkinkan untuk membangun model secara cepat dari training set yang berukuran besar. \u2022 Decision tree dengan ukuran tree yang kecil relatif mudah untuk menginterpretasinya. \u2022 Decision tree memberikan gambaran yang ekpresif dalam pembelajaran fungsi nilai diskret. \u2022 Algoritma decision tree cukup robbust terhadap munculnya noise terutama untuk metode yang dapat menangani masalah overfitting. \u2022 Adanya atribut yang berlebihan tidak terlalu mengurangi akurasi decision tree . \u2022 Karena sebagian algoritma decision tree menggunakan pendekatan topdown, yaitu partisi dilakukan secara rekursif maka jumlah record menjadi lebih kecil. Pada leaf node, jumlah record mungkin akan terlalu kecil untuk dapat membuat keputusan secara statistik tentang representasi kelas dari suatu node. \u2022 Sebuah subtree dapat direplikasi beberapa kali dalam decision tree tetapi ini akan menyebabkan decision tree menjadi lebih kompleks dan lebih sulit untuk diinterpretasi. B. ALGORITMA ID3 \u00b6 Algoritma ID3 merupakan algoritma yang dipergunakan untuk membangun sebuah decision tree atau pohon keputusan. Algoritma ini ditemukan oleh J. Ross Quinlan (1979), dengan memanfaatkan Teori Informasi atau Information Theory milik Shanon. ID3 sendiri merupakan singkatan dari Iterative Dichotomiser 3. Decision tree menggunakan struktur hierarki untuk pembelajaran supervised. Proses dari decision tree dimulai dari root node hingga leaf node yang dilakukan secara rekursif. Di mana setiap percabangan menyatakan suatu kondisi yang harus dipenuhi dan pada setiap ujung pohon menyatakan kelas dari suatu data. Proses dalam decision tree yaitu mengubah bentuk data (tabel) menjadi model pohon (tree) kemudian mengubah model pohon tersebut menjadi aturan (rule). C. ARSITEKTUR DECISION TREE \u00b6 Arsitektur pohon keputusan dibuat menyerupai bentuk pohon, dimana pada umumnya sebuah pohon terdapat akar (root), cabang dan daun (leaf). Pada pohon keputusan juga terdiri dari tiga bagian sebagai berikut : a. Root node atau node akar merupakan node yang terletak paling atas dari suatu pohon. b. Internal Node ini merupakan node percabangan, dimana pada node ini hanya terdapat satu input dan mempunyai minimal dua output. c. Leaf Node ini merupakan node akhir, hanya memiliki satu input, dan tidak memiliki output. Pada pohon keputusan setiap leaf node menandai label kelas. Pada pohon keputusan di setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan nilai kelas data. Gambar berikut merupakan bentuk arsitektur pohon keputusan. Pemilihan atribut untuk menjadi rootnode atau internal node sebagai atribut test berdasarkan atas ukuran impurity dari masing\u2013masing atribut. Ukuran\u2013ukuran impurity yang umumnya digunakan adalah information gain, gain ratio dan gini index. Atribut yang memiliki nilai impurity tertinggi akan dipilih sebagai atribut test. Langkah-Langkah Konstruksi Pohon Keputusan dengan Algoritma ID3 \u00b6 Adapun langkah-langkah dalam konstruksi pohon keputusan adalah sebagai berikut : Langkah 1 : Pohon dimulai dengan sebuah simpul yang mereperesentasikan sampel data pelatihan yaitu dengan membuat simpul akar. Langkah 2 : Jika semua sampel berada dalam kelas yang sama, maka simpul ini menjadi daun dan dilabeli menjadi kelas. Jika tidak, information gain akan digunakan untuk memilih atribut terbaik dalam memisahkan data sampel menjadi kelas-kelas individu. Langkah 3 : Cabang akan dibuat untuk setiap nilai pada atribut dan data sampel akan dipartisi lagi. Langkah 4 : Algoritma ini menggunakan proses rekursif untuk membentuk pohon keputusan pada setiap data partisi. Jika sebuah atribut sduah digunakan disebuah simpul, maka atribut ini tidak akan digunakan lagi di simpul anak-anaknya. Langkah 5 : Proses ini berhenti jika dicapai kondisi seperti berikut : \u2013 Semua sampel pada simpul berada di dalam satu kelas \u2013 Tidak ada atribut lainnya yang dapat digunakan untuk mempartisi sampel lebih lanjut. Dalam hal ini akan diterapkan suara terbanyak. Ini berarti mengubah sebuah simpul menjadi daun dan melabelinya dnegan kelas pada suara terbanyak. Entropy & Information Gain \u00b6 Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. $$ \\begin{equation} (S)=\\sum_{j=1}^{k}-p_{j} \\log {2} p {j} \\end{equation} $$ Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. $$ \\begin{equation} \\operatorname{Gain}(A)=E n t r o p i(S)-\\sum_{i=1}^{k} \\frac{\\left|S_{i}\\right|}{|S|} \\times E n t r o p i\\left(S_{i}\\right) \\end{equation} $$ Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i D. PERHITUNGAN SEDERHANA DECISION TREE \u00b6 Contoh \u00b6 Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca , Suhu , Kelembaban , dan Berangin . Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201c Tidak \u201d dan kelas \u201c Ya \u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. NO CUACA SUHU KELEMBAPAN BERANGIN MAIN 1 cerah panas tinggi salah tidak 2 cerah panas tinggi benar tidak 3 berawan panas tinggi salah ya 4 hujan sejuk tinggi salah ya 5 hujan dingin normal salah ya 6 hujan dingin normal benar ya 7 berawan dingin normal benar ya 8 cerah sejuk tinggi salah tidak 9 cerah dingin normal salah ya 10 hujan sejuk normal salah ya 11 cerah sejuk normal benar ya 12 berawan sejuk tinggi benar ya 13 berawan panas normal salah ya 14 hujan sejuk normal benar tidak Penyelesaian \u00b6 hitung entropi *Entropi (S) = (-(10/14) x log*2 *(10/14) + (-(4/10) x log*2 (4/10)) = 0.863120569 Total Kasus sum(YA) sum(TIDAK) Entropi Total 14 10 4 0.863120569 Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037 Hitung pula Gain (Suhu), Gain (Kelembaban), dan Gain (Berangin). Karena nilai gain terbesar adalah Gain (Kelembaban), maka atribut \u201cKelembaban\u201d menjadi node akar (root node) . Kemudian pada \u201cKelembaban\u201d normal, memiliki 7 kasus dan semuanya memiliki jawaban Ya (Sum(Total) / Sum(Ya) = 7/7 = 1). Dengan demikian \u201cKelembaban\u201d normal menjadi daun atau leaf . Berdasarkan pembentukan pohon keputusan node 1 (root node), Node 1.1 akan dianalisis lebih lanjut. Untuk mempermudah, Tabel dibawah difilter, dengan mengambil data yang memiliki \u201cKelembaban\u201d = Tinggi. Kemudian dihitung nilai entropi atribut \u201cKelembaban\u201d Tinggi dan entropi setiap atribut serta gainnya. Setelah itu tentukan pilih atribut yang memiliki gain tertinggi untuk dibuatkan node berikutnya. Gain tertinggi yang didapat ada pada atribut \u201cCuaca\u201d, dan Nilai yang dijadikan daun atau leaf adalah Berawan dan Cerah. Jika divualisasi maka pohon keputusan tampak seperti Gambar dibawah. Untuk menganalisis node 1.1.2, lakukan lagi langkah-langkah yang sama seperti sebelumnya hingga semua node beberntuk node leaf . E. KELEBIHAN DAN KEKURANGAN DECISION TREE \u00b6 KELEBIHAN \u00b6 Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi simple dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka contoh diuji hanya berdasarkan kriteria atau kelas-kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan kriteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. KEKURANGAN \u00b6 Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN \u00b6 Dataset \u00b6 Judul: Haberman's Survival Data Sources: (a) Donor: Tjen-Sien Lim ( limt@stat.wisc.edu ) (b) Date: March 4, 1999 Jumlah Instances: 306 Jumlah Attributes: 4 (termasuk attribute class) Informasi Attribute: Age of patient at time of operation (numerical) Patient's year of operation (year - 1900, numerical) Number of positive axillary nodes detected (numerical) Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year Implementasi \u00b6 Langkah 1: Import Librari Python Machine Learning \u00b6 Bagian ini melibatkan mengimpor semua perpustakaan yang akan kita gunakan. Kami mengimpor modul numpy dan sklearn train_test_split, DecisionTreeClassifier & akurasi_score. import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn import tree Numpy array dan panda dataframe akan membantu kita dalam memanipulasi data. Seperti dibahas di atas, sklearn adalah perpustakaan pembelajaran mesin. Metode cross_validation train_test_split () akan membantu dengan memecah data menjadi train & test set. Modul tree akan digunakan untuk membangun Decision Tree Classifier. Modul Accutacy_score akan digunakan untuk menghitung metrik akurasi dari variabel kelas yang diprediksi. Langkah 2: Data Import \u00b6 Untuk mengimpor data dan memanipulasinya, kita akan menggunakan kerangka data panda. Pertama-tama, kita perlu mengunduh dataset. Anda dapat mengunduh dataset dari sini. Semua nilai data dipisahkan oleh koma. Setelah mengunduh file data, kami akan menggunakan metode Pandas read_csv () untuk mengimpor data ke dalam kerangka data panda. Karena data kami dipisahkan dengan koma \",\" dan tidak ada tajuk dalam data kami, jadi kami akan menempatkan nilai parameter \"None\" dan nilai parameter sep sebagai \",\". balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data' , sep = ',' , header = None ) Kami menyimpan data ke dalam variabel data \"balance_data\". Untuk memeriksa panjang & dimensi kerangka data, kami dapat menggunakan metode len () & \u201c.shape\u201d. print ( \"Dataset Lenght:: \" , len ( balance_data )) print ( \"Dataset Shape:: \" , balance_data . shape ) Output Kita dapat print head .e, menggunakan method head() atau method print itu sendiri print ( \"Dataset:: \" ) balance_data . head () #atau print (balance_data) Output Langkah 3: Data Slicing \u00b6 Data Slicing (Mengiris data) adalah langkah untuk membagi data menjadi set training dan testing. Kumpulan data pelatihan dapat digunakan secara khusus untuk pembangunan model. Dataset testing tidak boleh dicampuradukkan saat membangun model. Bahkan selama standardisasi, kita tidak boleh menstandarisasi set testing. X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] script di atas membagi data menjadi set fitur & target yang ditetapkan. Set \"X\" terdiri dari variabel prediktor. Ini terdiri dari data dari kolom 2 hingga kolom 5. Set \"Y\" terdiri dari variabel hasil. Ini terdiri dari data di kolom 1. Kami menggunakan \".values\" dari numpy yang mengubah dataframe kami menjadi array numpy. Mari kita pisahkan data kita menjadi set training dan testing. Kami akan menggunakan metode sklearn train_test_split (). X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) script di atas akan membagi data menjadi set training dan testing. X_train, y_train adalah data training & X_test, y_test milik dataset testing . Parameter test_size diberi nilai 0,3; itu berarti set tes akan menjadi 30% dari seluruh dataset & ukuran dataset training akan menjadi 70% dari seluruh dataset. variabel random_state adalah keadaan generator angka pseudo-acak yang digunakan untuk pengambilan sampel acak. Jika Anda ingin mereplikasi hasil kami, maka gunakan nilai random_state yang sama. Langkah 4: Decision Tree Training \u00b6 Sekarang kita mencocokan algoritma Decision Tree pada data training, memprediksi label untuk dataset validasi dan mencetak akurasi model menggunakan berbagai parameter. DecisionTreeClassifier (): Ini adalah fungsi classifier untuk DecisionTree merupakan fungsi utama untuk mengimplementasikan algoritma. Beberapa parameter penting adalah: Criterion : Ini mendefinisikan fungsi untuk mengukur kualitas split. Sklearn mendukung kriteria \"gini\" untuk Indeks Gini & \"entropi\" untuk Penguatan Informasi. Secara default, dibutuhkan nilai \"gini\". splitter : Ini mendefinisikan strategi untuk memilih split pada setiap node. Mendukung nilai \"best\" untuk memilih split terbaik & \"random\" untuk memilih split acak terbaik. Secara default, dibutuhkan nilai \"best\". max_features : Ini menentukan no. fitur yang perlu dipertimbangkan ketika mencari perpecahan terbaik. Kami dapat memasukkan nilai integer, float, string & None. Jika integer dimasukkan, maka nilai tersebut dianggap sebagai fitur maksimal pada setiap pemisahan. Jika nilai float diambil maka ini menunjukkan persentase fitur di setiap split. Jika \"otomatis\" atau \"sqrt\" diambil maka max_features = sqrt (n_features). Jika \"log2\" diambil maka max_features = log2 (n_features). Jika None, maka max_features = n_features. Secara default, dibutuhkan nilai \"none\". max_depth : Parameter max_depth menunjukkan kedalaman maksimum pohon. Itu bisa mengambil nilai integer atau none. Jika none, maka node diperluas sampai semua daun murni atau sampai semua daun mengandung kurang dari sampel min_samples_split. Secara default, dibutuhkan nilai \"None\". min_samples_split : Ini memberitahu di atas no minimum. sampel reqd. untuk membagi simpul internal. Jika nilai integer diambil maka pertimbangkan min_samples_split sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai \"2\". min_samples_leaf : Jumlah sampel minimum yang diperlukan berada pada simpul daun. Jika nilai integer diambil maka pertimbangkan min_samples_leaf sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai \u201c1\u201d. max_leaf_nodes : Ini mendefinisikan jumlah maksimum dari node leaf yang mungkin. Jika none maka dibutuhkan jumlah daun yang tidak terbatas. Secara default, dibutuhkan nilai \"None\". min_impurity_split : Ini mendefinisikan ambang batas untuk menghentikan pertumbuhan pohon awal. Suatu simpul akan terpecah jika kenajisannya berada di atas ambang batas kalau tidak itu adalah daun. Mari kita buat pengklasifikasi menggunakan kriteria sebagai indeks gini & keuntungan informasi. Kita harus menyesuaikan classifier kita menggunakan fit (). Kami akan memplot visualisasi classifier pohon keputusan kami juga. Decision Tree Classifier dengan kriteria GINI clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 306 , max_depth = 3 , min_samples_leaf = 5 ) clf_gini . fit ( X_train , y_train ) DecisionTreeClassifier ( class_weight = None , criterion = 'gini' , max_depth = 10 , max_features = None , max_leaf_nodes = None , min_samples_leaf = 5 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 306 , splitter = 'best' ) Output Decision Tree Classifier dengan kriteria information gain clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 306 , max_depth = 3 , min_samples_leaf = 5 ) clf_entropy . fit ( X_train , y_train ) DecisionTreeClassifier ( class_weight = None , criterion = 'entropy' , max_depth = 3 , max_features = None , max_leaf_nodes = None , min_samples_leaf = 5 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 306 , splitter = 'best' ) Output Langkah 5: Prediction \u00b6 Sekarang, kami telah memodelkan 2 pengklasifikasi. Satu classifier dengan indeks gini & satu lagi dengan information gain sebagai kriteria. Kami siap memprediksi kelas untuk set testing kami. Kita dapat menggunakan metode predict (). Mari kita coba memprediksi variabel target untuk catatan pertama kelompok testing. clf_gini . predict ([[ 4 , 4 , 3 , 3 ]]) Dengan cara ini kita dapat memprediksi kelas untuk satu catatan. Sudah waktunya untuk memprediksi variabel target untuk seluruh dataset pengujian. Prediction untuk Decision Tree classifier dengan criterion sebagai gini index \u00b6 y_pred = clf_gini . predict ( X_test ) print ( y_pred ) Output Prediction untuk Decision Tree classifier dengan criterion sebagai information gain \u00b6 y_pred_en = clf_entropy . predict ( X_test ) print ( y_pred_en ) Output Langkah 6: Menghitung Score Akurasi \u00b6 Function akurasi_score () akan digunakan untuk mencetak akurasi algoritma Decision Tree. Secara akurat, maksud kami adalah rasio titik data yang diprediksi dengan benar dengan semua titik data yang diprediksi. Akurasi sebagai matrik membantu memahami efektivitas algoritme . Dibutuhkan 4 parameter. y_true, y_pred, Normalize, sample_weight. Dari 4 ini, normalisasi & sample_weight adalah parameter opsional. Parameter y_true menerima larik label yang benar dan y_pred mengambil larik label yang diprediksi yang dikembalikan oleh classifier. Ini mengembalikan akurasi sebagai nilai float. Accuracy untuk Decision Tree classifier dengan criterion sebagai gini index \u00b6 print ( \"Accuracy gini index \" , accuracy_score ( y_test , y_pred ) * 100 ) Output Accuracy untuk Decision Tree classifier denga criterion sebagai information gain \u00b6 print ( \"Accuracy information gain \" , accuracy_score ( y_test , y_pred_en ) * 100 ) Output REFERENSI \u00b6 https://informatikalogi.com/algoritma-id3/ https://sinta.unud.ac.id/uploads/wisuda/1108605014-3-Bab%20II.pdf https://www.academia.edu/35688101/MAKALAH_KLASIFIKASI_DECISION_TREE https://garudacyber.co.id/artikel/1545-pengertian-dan-penerapan-decision-tree http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/","title":"DECISION TREE CLASSIFIER"},{"location":"decision-tree/#a-penjelasan-decision-tree","text":"Decision tree atau pohon keputusan adalah alat pendukung keputusan yang menggunakan model keputusan yang berbentuk seperti pohon. Decision tree memetakan berbagai alternatif yang mungkin untuk mengatasi suatu masalah, dan terdapat juga faktor-faktor kemungkinan yang dapat mempengaruhi alternatif tersebut beserta estimasi akhirnya jika memilih alternatif yang ada. Decision tree merupakan salah satu metode yang bisa digunaan untuk menampilkan algoritma dimana hanya berisi pernyataan kontrol bersyarat. Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas\u2013kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu","title":"A. PENJELASAN DECISION TREE"},{"location":"decision-tree/#decision-tree-terdiri-dari-tiga-jenis-simpul","text":"Simpul keputusan - biasanya diwakili oleh kotak Simpul peluang - biasanya diwakili oleh lingkaran Simpul akhir - biasanya diwakili oleh segitiga","title":"Decision tree terdiri dari tiga jenis simpul:"},{"location":"decision-tree/#konsep-decision-tree","text":"Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu.","title":"Konsep Decision Tree"},{"location":"decision-tree/#karakteristik-decision-tree","text":"Berikut ini adalah beberapa karakteristik decision tree secara umum : \u2022 Decision tree merupakan suatu pendekatan nonparametrik untuk membangun model klasifikasi \u2022 Teknik yang dikembangkan dalam membangun decision tree memungkinkan untuk membangun model secara cepat dari training set yang berukuran besar. \u2022 Decision tree dengan ukuran tree yang kecil relatif mudah untuk menginterpretasinya. \u2022 Decision tree memberikan gambaran yang ekpresif dalam pembelajaran fungsi nilai diskret. \u2022 Algoritma decision tree cukup robbust terhadap munculnya noise terutama untuk metode yang dapat menangani masalah overfitting. \u2022 Adanya atribut yang berlebihan tidak terlalu mengurangi akurasi decision tree . \u2022 Karena sebagian algoritma decision tree menggunakan pendekatan topdown, yaitu partisi dilakukan secara rekursif maka jumlah record menjadi lebih kecil. Pada leaf node, jumlah record mungkin akan terlalu kecil untuk dapat membuat keputusan secara statistik tentang representasi kelas dari suatu node. \u2022 Sebuah subtree dapat direplikasi beberapa kali dalam decision tree tetapi ini akan menyebabkan decision tree menjadi lebih kompleks dan lebih sulit untuk diinterpretasi.","title":"Karakteristik Decision Tree"},{"location":"decision-tree/#b-algoritma-id3","text":"Algoritma ID3 merupakan algoritma yang dipergunakan untuk membangun sebuah decision tree atau pohon keputusan. Algoritma ini ditemukan oleh J. Ross Quinlan (1979), dengan memanfaatkan Teori Informasi atau Information Theory milik Shanon. ID3 sendiri merupakan singkatan dari Iterative Dichotomiser 3. Decision tree menggunakan struktur hierarki untuk pembelajaran supervised. Proses dari decision tree dimulai dari root node hingga leaf node yang dilakukan secara rekursif. Di mana setiap percabangan menyatakan suatu kondisi yang harus dipenuhi dan pada setiap ujung pohon menyatakan kelas dari suatu data. Proses dalam decision tree yaitu mengubah bentuk data (tabel) menjadi model pohon (tree) kemudian mengubah model pohon tersebut menjadi aturan (rule).","title":"B. ALGORITMA ID3"},{"location":"decision-tree/#c-arsitektur-decision-tree","text":"Arsitektur pohon keputusan dibuat menyerupai bentuk pohon, dimana pada umumnya sebuah pohon terdapat akar (root), cabang dan daun (leaf). Pada pohon keputusan juga terdiri dari tiga bagian sebagai berikut : a. Root node atau node akar merupakan node yang terletak paling atas dari suatu pohon. b. Internal Node ini merupakan node percabangan, dimana pada node ini hanya terdapat satu input dan mempunyai minimal dua output. c. Leaf Node ini merupakan node akhir, hanya memiliki satu input, dan tidak memiliki output. Pada pohon keputusan setiap leaf node menandai label kelas. Pada pohon keputusan di setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan nilai kelas data. Gambar berikut merupakan bentuk arsitektur pohon keputusan. Pemilihan atribut untuk menjadi rootnode atau internal node sebagai atribut test berdasarkan atas ukuran impurity dari masing\u2013masing atribut. Ukuran\u2013ukuran impurity yang umumnya digunakan adalah information gain, gain ratio dan gini index. Atribut yang memiliki nilai impurity tertinggi akan dipilih sebagai atribut test.","title":"C. ARSITEKTUR DECISION TREE"},{"location":"decision-tree/#langkah-langkah-konstruksi-pohon-keputusan-dengan-algoritma-id3","text":"Adapun langkah-langkah dalam konstruksi pohon keputusan adalah sebagai berikut : Langkah 1 : Pohon dimulai dengan sebuah simpul yang mereperesentasikan sampel data pelatihan yaitu dengan membuat simpul akar. Langkah 2 : Jika semua sampel berada dalam kelas yang sama, maka simpul ini menjadi daun dan dilabeli menjadi kelas. Jika tidak, information gain akan digunakan untuk memilih atribut terbaik dalam memisahkan data sampel menjadi kelas-kelas individu. Langkah 3 : Cabang akan dibuat untuk setiap nilai pada atribut dan data sampel akan dipartisi lagi. Langkah 4 : Algoritma ini menggunakan proses rekursif untuk membentuk pohon keputusan pada setiap data partisi. Jika sebuah atribut sduah digunakan disebuah simpul, maka atribut ini tidak akan digunakan lagi di simpul anak-anaknya. Langkah 5 : Proses ini berhenti jika dicapai kondisi seperti berikut : \u2013 Semua sampel pada simpul berada di dalam satu kelas \u2013 Tidak ada atribut lainnya yang dapat digunakan untuk mempartisi sampel lebih lanjut. Dalam hal ini akan diterapkan suara terbanyak. Ini berarti mengubah sebuah simpul menjadi daun dan melabelinya dnegan kelas pada suara terbanyak.","title":"Langkah-Langkah Konstruksi Pohon Keputusan dengan Algoritma ID3"},{"location":"decision-tree/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. $$ \\begin{equation} (S)=\\sum_{j=1}^{k}-p_{j} \\log {2} p {j} \\end{equation} $$ Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. $$ \\begin{equation} \\operatorname{Gain}(A)=E n t r o p i(S)-\\sum_{i=1}^{k} \\frac{\\left|S_{i}\\right|}{|S|} \\times E n t r o p i\\left(S_{i}\\right) \\end{equation} $$ Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i","title":"Entropy &amp; Information Gain"},{"location":"decision-tree/#d-perhitungan-sederhana-decision-tree","text":"","title":"D. PERHITUNGAN SEDERHANA DECISION TREE"},{"location":"decision-tree/#contoh","text":"Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca , Suhu , Kelembaban , dan Berangin . Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201c Tidak \u201d dan kelas \u201c Ya \u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. NO CUACA SUHU KELEMBAPAN BERANGIN MAIN 1 cerah panas tinggi salah tidak 2 cerah panas tinggi benar tidak 3 berawan panas tinggi salah ya 4 hujan sejuk tinggi salah ya 5 hujan dingin normal salah ya 6 hujan dingin normal benar ya 7 berawan dingin normal benar ya 8 cerah sejuk tinggi salah tidak 9 cerah dingin normal salah ya 10 hujan sejuk normal salah ya 11 cerah sejuk normal benar ya 12 berawan sejuk tinggi benar ya 13 berawan panas normal salah ya 14 hujan sejuk normal benar tidak","title":"Contoh"},{"location":"decision-tree/#penyelesaian","text":"hitung entropi *Entropi (S) = (-(10/14) x log*2 *(10/14) + (-(4/10) x log*2 (4/10)) = 0.863120569 Total Kasus sum(YA) sum(TIDAK) Entropi Total 14 10 4 0.863120569 Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037 Hitung pula Gain (Suhu), Gain (Kelembaban), dan Gain (Berangin). Karena nilai gain terbesar adalah Gain (Kelembaban), maka atribut \u201cKelembaban\u201d menjadi node akar (root node) . Kemudian pada \u201cKelembaban\u201d normal, memiliki 7 kasus dan semuanya memiliki jawaban Ya (Sum(Total) / Sum(Ya) = 7/7 = 1). Dengan demikian \u201cKelembaban\u201d normal menjadi daun atau leaf . Berdasarkan pembentukan pohon keputusan node 1 (root node), Node 1.1 akan dianalisis lebih lanjut. Untuk mempermudah, Tabel dibawah difilter, dengan mengambil data yang memiliki \u201cKelembaban\u201d = Tinggi. Kemudian dihitung nilai entropi atribut \u201cKelembaban\u201d Tinggi dan entropi setiap atribut serta gainnya. Setelah itu tentukan pilih atribut yang memiliki gain tertinggi untuk dibuatkan node berikutnya. Gain tertinggi yang didapat ada pada atribut \u201cCuaca\u201d, dan Nilai yang dijadikan daun atau leaf adalah Berawan dan Cerah. Jika divualisasi maka pohon keputusan tampak seperti Gambar dibawah. Untuk menganalisis node 1.1.2, lakukan lagi langkah-langkah yang sama seperti sebelumnya hingga semua node beberntuk node leaf .","title":"Penyelesaian"},{"location":"decision-tree/#e-kelebihan-dan-kekurangan-decision-tree","text":"","title":"E. KELEBIHAN DAN KEKURANGAN DECISION TREE"},{"location":"decision-tree/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi simple dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka contoh diuji hanya berdasarkan kriteria atau kelas-kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan kriteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"KELEBIHAN"},{"location":"decision-tree/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"KEKURANGAN"},{"location":"decision-tree/#f-implementasi-decision-tree-haberman-dataset-menggunakan-python-scikit-learn","text":"","title":"F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN"},{"location":"decision-tree/#dataset","text":"Judul: Haberman's Survival Data Sources: (a) Donor: Tjen-Sien Lim ( limt@stat.wisc.edu ) (b) Date: March 4, 1999 Jumlah Instances: 306 Jumlah Attributes: 4 (termasuk attribute class) Informasi Attribute: Age of patient at time of operation (numerical) Patient's year of operation (year - 1900, numerical) Number of positive axillary nodes detected (numerical) Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year","title":"Dataset"},{"location":"decision-tree/#implementasi","text":"","title":"Implementasi"},{"location":"decision-tree/#langkah-1-import-librari-python-machine-learning","text":"Bagian ini melibatkan mengimpor semua perpustakaan yang akan kita gunakan. Kami mengimpor modul numpy dan sklearn train_test_split, DecisionTreeClassifier & akurasi_score. import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn import tree Numpy array dan panda dataframe akan membantu kita dalam memanipulasi data. Seperti dibahas di atas, sklearn adalah perpustakaan pembelajaran mesin. Metode cross_validation train_test_split () akan membantu dengan memecah data menjadi train & test set. Modul tree akan digunakan untuk membangun Decision Tree Classifier. Modul Accutacy_score akan digunakan untuk menghitung metrik akurasi dari variabel kelas yang diprediksi.","title":"Langkah 1: Import Librari Python Machine Learning"},{"location":"decision-tree/#langkah-2-data-import","text":"Untuk mengimpor data dan memanipulasinya, kita akan menggunakan kerangka data panda. Pertama-tama, kita perlu mengunduh dataset. Anda dapat mengunduh dataset dari sini. Semua nilai data dipisahkan oleh koma. Setelah mengunduh file data, kami akan menggunakan metode Pandas read_csv () untuk mengimpor data ke dalam kerangka data panda. Karena data kami dipisahkan dengan koma \",\" dan tidak ada tajuk dalam data kami, jadi kami akan menempatkan nilai parameter \"None\" dan nilai parameter sep sebagai \",\". balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data' , sep = ',' , header = None ) Kami menyimpan data ke dalam variabel data \"balance_data\". Untuk memeriksa panjang & dimensi kerangka data, kami dapat menggunakan metode len () & \u201c.shape\u201d. print ( \"Dataset Lenght:: \" , len ( balance_data )) print ( \"Dataset Shape:: \" , balance_data . shape ) Output Kita dapat print head .e, menggunakan method head() atau method print itu sendiri print ( \"Dataset:: \" ) balance_data . head () #atau print (balance_data) Output","title":"Langkah 2: Data Import"},{"location":"decision-tree/#langkah-3-data-slicing","text":"Data Slicing (Mengiris data) adalah langkah untuk membagi data menjadi set training dan testing. Kumpulan data pelatihan dapat digunakan secara khusus untuk pembangunan model. Dataset testing tidak boleh dicampuradukkan saat membangun model. Bahkan selama standardisasi, kita tidak boleh menstandarisasi set testing. X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] script di atas membagi data menjadi set fitur & target yang ditetapkan. Set \"X\" terdiri dari variabel prediktor. Ini terdiri dari data dari kolom 2 hingga kolom 5. Set \"Y\" terdiri dari variabel hasil. Ini terdiri dari data di kolom 1. Kami menggunakan \".values\" dari numpy yang mengubah dataframe kami menjadi array numpy. Mari kita pisahkan data kita menjadi set training dan testing. Kami akan menggunakan metode sklearn train_test_split (). X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) script di atas akan membagi data menjadi set training dan testing. X_train, y_train adalah data training & X_test, y_test milik dataset testing . Parameter test_size diberi nilai 0,3; itu berarti set tes akan menjadi 30% dari seluruh dataset & ukuran dataset training akan menjadi 70% dari seluruh dataset. variabel random_state adalah keadaan generator angka pseudo-acak yang digunakan untuk pengambilan sampel acak. Jika Anda ingin mereplikasi hasil kami, maka gunakan nilai random_state yang sama.","title":"Langkah 3: Data Slicing"},{"location":"decision-tree/#langkah-4-decision-tree-training","text":"Sekarang kita mencocokan algoritma Decision Tree pada data training, memprediksi label untuk dataset validasi dan mencetak akurasi model menggunakan berbagai parameter. DecisionTreeClassifier (): Ini adalah fungsi classifier untuk DecisionTree merupakan fungsi utama untuk mengimplementasikan algoritma. Beberapa parameter penting adalah: Criterion : Ini mendefinisikan fungsi untuk mengukur kualitas split. Sklearn mendukung kriteria \"gini\" untuk Indeks Gini & \"entropi\" untuk Penguatan Informasi. Secara default, dibutuhkan nilai \"gini\". splitter : Ini mendefinisikan strategi untuk memilih split pada setiap node. Mendukung nilai \"best\" untuk memilih split terbaik & \"random\" untuk memilih split acak terbaik. Secara default, dibutuhkan nilai \"best\". max_features : Ini menentukan no. fitur yang perlu dipertimbangkan ketika mencari perpecahan terbaik. Kami dapat memasukkan nilai integer, float, string & None. Jika integer dimasukkan, maka nilai tersebut dianggap sebagai fitur maksimal pada setiap pemisahan. Jika nilai float diambil maka ini menunjukkan persentase fitur di setiap split. Jika \"otomatis\" atau \"sqrt\" diambil maka max_features = sqrt (n_features). Jika \"log2\" diambil maka max_features = log2 (n_features). Jika None, maka max_features = n_features. Secara default, dibutuhkan nilai \"none\". max_depth : Parameter max_depth menunjukkan kedalaman maksimum pohon. Itu bisa mengambil nilai integer atau none. Jika none, maka node diperluas sampai semua daun murni atau sampai semua daun mengandung kurang dari sampel min_samples_split. Secara default, dibutuhkan nilai \"None\". min_samples_split : Ini memberitahu di atas no minimum. sampel reqd. untuk membagi simpul internal. Jika nilai integer diambil maka pertimbangkan min_samples_split sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai \"2\". min_samples_leaf : Jumlah sampel minimum yang diperlukan berada pada simpul daun. Jika nilai integer diambil maka pertimbangkan min_samples_leaf sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai \u201c1\u201d. max_leaf_nodes : Ini mendefinisikan jumlah maksimum dari node leaf yang mungkin. Jika none maka dibutuhkan jumlah daun yang tidak terbatas. Secara default, dibutuhkan nilai \"None\". min_impurity_split : Ini mendefinisikan ambang batas untuk menghentikan pertumbuhan pohon awal. Suatu simpul akan terpecah jika kenajisannya berada di atas ambang batas kalau tidak itu adalah daun. Mari kita buat pengklasifikasi menggunakan kriteria sebagai indeks gini & keuntungan informasi. Kita harus menyesuaikan classifier kita menggunakan fit (). Kami akan memplot visualisasi classifier pohon keputusan kami juga. Decision Tree Classifier dengan kriteria GINI clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 306 , max_depth = 3 , min_samples_leaf = 5 ) clf_gini . fit ( X_train , y_train ) DecisionTreeClassifier ( class_weight = None , criterion = 'gini' , max_depth = 10 , max_features = None , max_leaf_nodes = None , min_samples_leaf = 5 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 306 , splitter = 'best' ) Output Decision Tree Classifier dengan kriteria information gain clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 306 , max_depth = 3 , min_samples_leaf = 5 ) clf_entropy . fit ( X_train , y_train ) DecisionTreeClassifier ( class_weight = None , criterion = 'entropy' , max_depth = 3 , max_features = None , max_leaf_nodes = None , min_samples_leaf = 5 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 306 , splitter = 'best' ) Output","title":"Langkah 4: Decision Tree Training"},{"location":"decision-tree/#langkah-5-prediction","text":"Sekarang, kami telah memodelkan 2 pengklasifikasi. Satu classifier dengan indeks gini & satu lagi dengan information gain sebagai kriteria. Kami siap memprediksi kelas untuk set testing kami. Kita dapat menggunakan metode predict (). Mari kita coba memprediksi variabel target untuk catatan pertama kelompok testing. clf_gini . predict ([[ 4 , 4 , 3 , 3 ]]) Dengan cara ini kita dapat memprediksi kelas untuk satu catatan. Sudah waktunya untuk memprediksi variabel target untuk seluruh dataset pengujian.","title":"Langkah 5: Prediction"},{"location":"decision-tree/#prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index","text":"y_pred = clf_gini . predict ( X_test ) print ( y_pred ) Output","title":"Prediction untuk Decision Tree classifier dengan criterion sebagai gini index"},{"location":"decision-tree/#prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-information-gain","text":"y_pred_en = clf_entropy . predict ( X_test ) print ( y_pred_en ) Output","title":"Prediction untuk Decision Tree classifier dengan criterion sebagai information gain"},{"location":"decision-tree/#langkah-6-menghitung-score-akurasi","text":"Function akurasi_score () akan digunakan untuk mencetak akurasi algoritma Decision Tree. Secara akurat, maksud kami adalah rasio titik data yang diprediksi dengan benar dengan semua titik data yang diprediksi. Akurasi sebagai matrik membantu memahami efektivitas algoritme . Dibutuhkan 4 parameter. y_true, y_pred, Normalize, sample_weight. Dari 4 ini, normalisasi & sample_weight adalah parameter opsional. Parameter y_true menerima larik label yang benar dan y_pred mengambil larik label yang diprediksi yang dikembalikan oleh classifier. Ini mengembalikan akurasi sebagai nilai float.","title":"Langkah 6: Menghitung Score Akurasi"},{"location":"decision-tree/#accuracy-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index","text":"print ( \"Accuracy gini index \" , accuracy_score ( y_test , y_pred ) * 100 ) Output","title":"Accuracy untuk Decision Tree classifier dengan criterion sebagai gini index"},{"location":"decision-tree/#accuracy-untuk-decision-tree-classifier-denga-criterion-sebagai-information-gain","text":"print ( \"Accuracy information gain \" , accuracy_score ( y_test , y_pred_en ) * 100 ) Output","title":"Accuracy untuk Decision Tree classifier denga  criterion sebagai information gain"},{"location":"decision-tree/#referensi","text":"https://informatikalogi.com/algoritma-id3/ https://sinta.unud.ac.id/uploads/wisuda/1108605014-3-Bab%20II.pdf https://www.academia.edu/35688101/MAKALAH_KLASIFIKASI_DECISION_TREE https://garudacyber.co.id/artikel/1545-pengertian-dan-penerapan-decision-tree http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/","title":"REFERENSI"},{"location":"knn/","text":"A. PENJELASAN K-NEAREST NEIGHBOR (KNN) \u00b6 Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. K-nearest neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori K-tetangga terdekat. KNN digunakan dalam banyak aplikasi data mining, statistical pattern recognition, image processing, dll. Beberapa aplikasinya meliputi: pengenalan tulisan tangan, satellite image dan ECG pattern. ECG produces apatternreflecting the electrical activity of the heart. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). B. ALGORITMA PERHITUNGAN KNN \u00b6 Langkah-Langkah untuk menghitung KNN \u00b6 Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru. Perhitungan Numerik KNN (Perhitungan dengan cara manual) \u00b6 dalam menghitung jarak ada 4 cara tergantung yang makai; Euclidean Distance $$ \\begin{equation} d(x, y)=\\sqrt{\\sum_{i=1} {m}\\left(x_{i}-y_{i}\\right) {2}} \\end{equation} $$ ide rumus ini dari pytaghoras $$ \\begin{equation} c=\\sqrt{a {2}+b {2}} \\end{equation} $$ d(x,y) dibaca distance antara x dan y Manhattan Distance $$ \\begin{equation} d(x, y)=\\sum_{i=1}^{m}\\left|x_{i}-y_{i}\\right| \\end{equation} $$ *rumus ini mencari jarak hanya dengan menjumlahkan semua selisih dari jarak dan . Mungkin idenya dari menghitung jarak dari 3 ke 5 yaitu 2 karena |3-5|=2. Apa bedanya dengan euclidean distance ? manhattan distance itu : \u200b Mengurangi per elemen antar 2 variabel, memutlakannya lalu menjumlahkannya. Sedangkan euclidean distance menghitung jarak antara 2 titik dengan konsep pythagoras. Minkowsky Distance $$ \\begin{equation} d(x, y)=\\left(\\sum_{i=1} {m}\\left|x_{i}-y_{i}\\right| {r}\\right)^{1 / r} \\end{equation} $$ ide rumus ini diambil dari konsep aljabar dengan objek vektor berdimensi n dan r bukan 1 dan 2. why? Karena kalau maka akan terbntuk manhattan distance, kalau euclidean distance. Chebychev Distance $$ \\begin{equation} d(x, y)=\\max {i=1}\\left|x {i}-y_{i}\\right| \\end{equation} $$ *rumus ini mencari jarak yang terbesar antara x_i dan y_i Fact : Algoritma ini adalah algoritma yang paling simpel dari semua algoritma machine learning. Fun : Algoritma ini bisa dipakai untuk \u201cmesin pencari\u201d seperti google. Contoh \u00b6 didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini. Penyelesaian \u00b6 Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3 Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) 7 7 7 4 3 4 1 4 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. kami menggunakan ecludian distance X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) Urutan (Ranking) Jarak Apakah termasuk K-NN? 7 7 3 YA 7 4 4 TIDAK 3 4 1 YA 1 4 2 YA Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) Urutan (Ranking) Jarak Apakah termasuk K-NN? Y=Category of Nearest Neighbor 7 7 3 YA Jelek 7 4 4 TIDAK - 3 4 1 YA Bagus 1 4 2 YA Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. \u2013Kita punya2 kategori Bagus dan1 kategori Jelek, karena2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus . C. KELEBIHAN DAN KEKURANGAN KNN \u00b6 Kelebihan \u00b6 Sangat nonlinear \u200b kNN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat nonparametrik. Pembahasan mengenai model parametrik dan model nonparametrik bisa menjadi artikel sendiri, namun secara singkat, definisi model nonparametrik adalah model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset. Model nonparametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan nonlinear. Mudah dipahami dan diimplementasikan \u200b Dari paparan yang diberikan dan penjelasan cara menghitung jarak dalam artikel ini, cukup jelas bahwa algoritma kNN mudah dipahami dan juga mudah dimplementasikan. Untuk mengklasifikasi instance x menggunakan kNN, kita cukup mendefinisikan fungsi untuk menghitung jarak antar-instance, menghitung jarak x dengan semua instance lainnya berdasarkan fungsi tersebut, dan menentukan kelas x sebagai kelas yang paling banyak muncul dalam k instance terdekat. Kekurangan \u00b6 Perlu menunjukkan parameter K (jumlah tetangga terdekat) Tidak menangani nilai hilang (missing value) secara implisit \u200b Jika terdapat nilai hilang pada satu atau lebih variabel dari suatu instance, perhitungan jarak instance tersebut dengan instance lainnya menjadi tidak terdefinisi. Bagaimana coba, menghitung jarak dalam ruang 3-dimensi jika salah satu dimensi hilang? Karenanya, sebelum menerapkan kNN kerap dilakukan imputasi untuk mengisi nilai-nilai hilang yang ada pada dataset. Contoh teknik imputasi yang paling umum adalah mengisi nilai hilang pada suatu variabel dengan nilai rata-rata variabel tersebut (mean imputation). Sensitif terhadap data pencilan (outlier) \u200b Seperti yang telah dijelaskan Ali pada artikel sebelumnya, kNN bisa jadi sangat fleksibel jika k kecil. Fleksibilitas ini mengakibatkan kNN cenderung sensitif terhadap data pencilan, khususnya pencilan yang terletak di \u201ctengah-tengah\u201d kelas yang berbeda. Lebih jelasnya, perhatikan ilustrasi di bawah. Pada gambar kiri, seluruh instance bisa diklasifikasikan dengan benar ke dalam kelas biru dan jingga. Tetapi, ketika ditambahkan instance biru di antara instance jingga, beberapa instance jingga menjadi salah terklasifikasi.Perlu dipilih k yang tepat untuk mengurangi dampak data pencilan dalam kNN. Rentan terhadap variabel yang non-informatif \u200b Meskipun kita telah menstandardisasi rentang variabel, kNN tetap tidak dapat mengetahui variabel mana yang signifikan dalam klasifikasi dan mana yang tidak Rentan terhadap dimensionalitas yang tinggi \u200b Berbagai permasalahan yang timbul dari tingginya dimensionalitas (baca: banyaknya variabel) menimpa sebagian besar algoritma pembelajaran mesin, dan kNN adalah salah satu algoritma yang paling rentan terhadap tingginya dimensionalitas. Hal ini karena semakin banyak dimensi, ruang yang bisa ditempati instance semakin besar, sehingga semakin besar pula kemungkinan bahwa nearest neighbour dari suatu instance sebetulnya sama sekali tidak \u201cnear\u201c. Rentan terhadap perbedaan rentang variabel \u200b Dalam perhitungan jarak antar-instance, kNN menganggap semua variabel setara atau sama penting (lihat bagian penjumlahan pada rumus perhitungan jarak di atas). Jika terdapat variabel p yang memiliki rentang jauh lebih besar dibanding variabel-variabel lainnya, maka perhitungan jarak akan didominasi oleh p. Misalkan ada dua variabel, a dan b, dengan rentang variabel a 0 sampai 1.000 dan rentang variabel b 0 sampai 10. Kuadrat selisih dua nilai variabel b tidak akan lebih dari 100, sedangkan untuk variabel a kuadrat selisihnya bisa mencapai 1.000.000. Hal ini bisa mengecoh kNN sehingga kNN menganggap a tidak membawa pengaruh dalam perhitungan jarak karena rentangnya sangat besar dibanding rentang b.Ilustrasinya diberikan di bawah ini. Manakah yang merupakan nearest neighbour dari instance x? Jika dilihat dari \u201ckacamata\u201d komputer, nearest neighbour x bukanlah y, melainkan z, Mengapa? Untuk mengatasi perbedaan rentang, biasanya dilakukan preproses berupa standardisasi rentang semua variabel sebelum menerapkan algoritma kNN Nilai komputasi yang tinggi. \u200b Untuk mengklasifikasi sebuah instance x, kNN harus menghitung jarak antara x dengan semua instance lain dalam dataset yang kita miliki. Dengan kata lain, kompleksitas waktu klasifikasi kNN berbanding lurus dengan jumlah instance latih. Jika dataset yang kita miliki berukuran besar (terdiri dari banyak instance dan/atau banyak variabel), proses ini bisa jadi sangat lambat. Bayangkan, jika kita punya 10.000 instance dengan masing-masing 20 variabel dan kita ingin mengklasifikasi 100 instance baru (instance uji), maka total operasi yang harus dilakukan menjadi: (100 instance uji x 10.000 instance latih) x 20 variabel/instance x 2 operasi/variabel = 40 juta operasi Beberapa cara pengindexan (K-D tree) dapat digunakan untuk mereduksi biaya komputasi. D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN \u00b6 Dataset \u00b6 Judul: Haberman's Survival Data Sources: (a) Donor: Tjen-Sien Lim ( limt@stat.wisc.edu ) (b) Date: March 4, 1999 Jumlah Instances: 306 Jumlah Attributes: 4 (termasuk attribute class) Informasi Attribute: Age of patient at time of operation (numerical) Patient's year of operation (year - 1900, numerical) Number of positive axillary nodes detected (numerical) Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year Implementasi \u00b6 Mengimport Librari \u00b6 Install terlebih dahulu librari-librari yang dibutuhkan. maka, module dapat di eksekusi import numpy as np import matplotlib.pyplot as plt import pandas as pd Langkah 1: Mengimport DataSet \u00b6 Untuk mengimport dan load data, anda dapat menggunkaan sintax dibawah ini: # memasukkan url dataset kedalam variabel, sehingga anda tidak perlu mendownload data. cukup memanggilnya url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\" # memasukkan nama kolom ke dalam dataset names = [ 'age' , 'patient-year-operation' , 'nodes-detected' , '(Survival-status)Class' ] # membaca dataset ke pandas dataframe dataset = pd . read_csv ( url , names = names ) Untuk melihat data, eksekusi code berikut: print ( dataset ) # atau dataset.head() Maka, akan menampilkan data seperti dibawah ini (row paling atas dan bawah): Langkah 2: Preprocessing \u00b6 Langkah selanjutnya adalah memisahkan dataset kedalam atribut dan label. gunakan code berikut ini: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 3 ] . values variabel X untuk mendefinisikan 3 Kolom pada dataset sedangkan variabel y untuk mendefinisikan label pada dataset. Langkah 3: Train Test Split \u00b6 Untuk memisahkan data menjadi data training dan data testing. performa algoritma yang saya gunakan selama fase testing tidak terlihat (un-seen data). berikut code untuk membuat training dan testing data: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) script diatas memisahkan dataset 80% data training dan 20% data testing. dari 306 data, terbagi data training sebesar 245 data dan data testing sebesar 61 data. Fitur Scaling \u00b6 Sebelum kita memprediksi data yang sebenarnya. alangkah baiknya kita menggunakan fitur scaling untuk menormalisasikan data sebelum di testing. berikut codenya: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Langkah 4: Training dan Prediction \u00b6 from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 7 ) classifier . fit ( X_train , y_train ) langkah pertama adlah mengimport librari KNeighborsClassifier class from the sklearn.neighbors . Pada baris kedua, class di inisialisasi dengan parameter, i.e. n_neigbours . nilai K yang diuji adalah K-7. Kemudian kita memprediksi nilai dari datset: y_pred = classifier . predict ( X_test ) Langkah 5: Evaluating dari Algoritma \u00b6 Untuk mengevaluasi sebuah algoritma, matrix, presisi, recall dan f1 score pada umumnya menggunnakan matrix dengan confusion_matrix dan classification_report methods dari sklearn.metrics Berikut code programnya: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) Tampilan output dari script diatas: dari hasil output diatas dapat diketahui bahwa nilai K-7 menghasilkan 71% Langkah 6: Mengekstrak rating error pada K Value \u00b6 Pada testing dan evaluasi data sebelumnya. kita telah mengecek akurasi pada K-. Namun, untuk menghasilkan nilai tingkat akurasi yang tinggi kita dapat mengecek beberapa sebagai acuan tanpa mengulang-ngulang data program yaitu dengan cara plot grafik dan coresponden nilai errror pada K. Untuk menghitung, nilai value saya menggunakan distance K11:40. anda dapat menggunakan script berikut: error = [] # menghitung nilai error pada K antara 1 dan 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) Script diatas mengeksekusi perulangan 1 sampai 40. pada setiap iterasi, prediksi nilai error yang telah dihitung akan masuk dalam list error. langkah selanjutnya adalah plot nilai error K. berikut script code: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = 'red' , linestyle = 'dashed' , marker = 'o' , markerfacecolor = 'blue' , markersize = 10 ) plt . title ( 'Rating Error Nilai K' ) plt . xlabel ( 'Nilai K' ) plt . ylabel ( 'Mean Error' ) Output REFERENSI \u00b6 https://id.wikipedia.org/wiki/KNN https://en.wikipedia.org/wiki/Feature_scaling https://belajarkalkulus.com/clustering-part-iii/ http://depandienda.it.student.pens.ac.id/file/knn_references.pdf https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/","title":"K-NEAREST NEIGHBOR CLASSIFIER"},{"location":"knn/#a-penjelasan-k-nearest-neighbor-knn","text":"Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. K-nearest neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori K-tetangga terdekat. KNN digunakan dalam banyak aplikasi data mining, statistical pattern recognition, image processing, dll. Beberapa aplikasinya meliputi: pengenalan tulisan tangan, satellite image dan ECG pattern. ECG produces apatternreflecting the electrical activity of the heart. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu).","title":"A. PENJELASAN K-NEAREST NEIGHBOR (KNN)"},{"location":"knn/#b-algoritma-perhitungan-knn","text":"","title":"B. ALGORITMA PERHITUNGAN KNN"},{"location":"knn/#langkah-langkah-untuk-menghitung-knn","text":"Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.","title":"Langkah-Langkah untuk menghitung KNN"},{"location":"knn/#perhitungan-numerik-knn-perhitungan-dengan-cara-manual","text":"dalam menghitung jarak ada 4 cara tergantung yang makai; Euclidean Distance $$ \\begin{equation} d(x, y)=\\sqrt{\\sum_{i=1} {m}\\left(x_{i}-y_{i}\\right) {2}} \\end{equation} $$ ide rumus ini dari pytaghoras $$ \\begin{equation} c=\\sqrt{a {2}+b {2}} \\end{equation} $$ d(x,y) dibaca distance antara x dan y Manhattan Distance $$ \\begin{equation} d(x, y)=\\sum_{i=1}^{m}\\left|x_{i}-y_{i}\\right| \\end{equation} $$ *rumus ini mencari jarak hanya dengan menjumlahkan semua selisih dari jarak dan . Mungkin idenya dari menghitung jarak dari 3 ke 5 yaitu 2 karena |3-5|=2. Apa bedanya dengan euclidean distance ? manhattan distance itu : \u200b Mengurangi per elemen antar 2 variabel, memutlakannya lalu menjumlahkannya. Sedangkan euclidean distance menghitung jarak antara 2 titik dengan konsep pythagoras. Minkowsky Distance $$ \\begin{equation} d(x, y)=\\left(\\sum_{i=1} {m}\\left|x_{i}-y_{i}\\right| {r}\\right)^{1 / r} \\end{equation} $$ ide rumus ini diambil dari konsep aljabar dengan objek vektor berdimensi n dan r bukan 1 dan 2. why? Karena kalau maka akan terbntuk manhattan distance, kalau euclidean distance. Chebychev Distance $$ \\begin{equation} d(x, y)=\\max {i=1}\\left|x {i}-y_{i}\\right| \\end{equation} $$ *rumus ini mencari jarak yang terbesar antara x_i dan y_i Fact : Algoritma ini adalah algoritma yang paling simpel dari semua algoritma machine learning. Fun : Algoritma ini bisa dipakai untuk \u201cmesin pencari\u201d seperti google.","title":"Perhitungan Numerik KNN (Perhitungan dengan cara manual)"},{"location":"knn/#contoh","text":"didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini.","title":"Contoh"},{"location":"knn/#penyelesaian","text":"Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3 Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) 7 7 7 4 3 4 1 4 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. kami menggunakan ecludian distance X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) Urutan (Ranking) Jarak Apakah termasuk K-NN? 7 7 3 YA 7 4 4 TIDAK 3 4 1 YA 1 4 2 YA Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1=Daya Tahan Kesamaan (detik) X2=Kekuatan (Kg/meter persegi) Square Distance ke Data baru (3,7) Urutan (Ranking) Jarak Apakah termasuk K-NN? Y=Category of Nearest Neighbor 7 7 3 YA Jelek 7 4 4 TIDAK - 3 4 1 YA Bagus 1 4 2 YA Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. \u2013Kita punya2 kategori Bagus dan1 kategori Jelek, karena2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus .","title":"Penyelesaian"},{"location":"knn/#c-kelebihan-dan-kekurangan-knn","text":"","title":"C. KELEBIHAN DAN KEKURANGAN KNN"},{"location":"knn/#kelebihan","text":"Sangat nonlinear \u200b kNN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat nonparametrik. Pembahasan mengenai model parametrik dan model nonparametrik bisa menjadi artikel sendiri, namun secara singkat, definisi model nonparametrik adalah model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset. Model nonparametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan nonlinear. Mudah dipahami dan diimplementasikan \u200b Dari paparan yang diberikan dan penjelasan cara menghitung jarak dalam artikel ini, cukup jelas bahwa algoritma kNN mudah dipahami dan juga mudah dimplementasikan. Untuk mengklasifikasi instance x menggunakan kNN, kita cukup mendefinisikan fungsi untuk menghitung jarak antar-instance, menghitung jarak x dengan semua instance lainnya berdasarkan fungsi tersebut, dan menentukan kelas x sebagai kelas yang paling banyak muncul dalam k instance terdekat.","title":"Kelebihan"},{"location":"knn/#kekurangan","text":"Perlu menunjukkan parameter K (jumlah tetangga terdekat) Tidak menangani nilai hilang (missing value) secara implisit \u200b Jika terdapat nilai hilang pada satu atau lebih variabel dari suatu instance, perhitungan jarak instance tersebut dengan instance lainnya menjadi tidak terdefinisi. Bagaimana coba, menghitung jarak dalam ruang 3-dimensi jika salah satu dimensi hilang? Karenanya, sebelum menerapkan kNN kerap dilakukan imputasi untuk mengisi nilai-nilai hilang yang ada pada dataset. Contoh teknik imputasi yang paling umum adalah mengisi nilai hilang pada suatu variabel dengan nilai rata-rata variabel tersebut (mean imputation). Sensitif terhadap data pencilan (outlier) \u200b Seperti yang telah dijelaskan Ali pada artikel sebelumnya, kNN bisa jadi sangat fleksibel jika k kecil. Fleksibilitas ini mengakibatkan kNN cenderung sensitif terhadap data pencilan, khususnya pencilan yang terletak di \u201ctengah-tengah\u201d kelas yang berbeda. Lebih jelasnya, perhatikan ilustrasi di bawah. Pada gambar kiri, seluruh instance bisa diklasifikasikan dengan benar ke dalam kelas biru dan jingga. Tetapi, ketika ditambahkan instance biru di antara instance jingga, beberapa instance jingga menjadi salah terklasifikasi.Perlu dipilih k yang tepat untuk mengurangi dampak data pencilan dalam kNN. Rentan terhadap variabel yang non-informatif \u200b Meskipun kita telah menstandardisasi rentang variabel, kNN tetap tidak dapat mengetahui variabel mana yang signifikan dalam klasifikasi dan mana yang tidak Rentan terhadap dimensionalitas yang tinggi \u200b Berbagai permasalahan yang timbul dari tingginya dimensionalitas (baca: banyaknya variabel) menimpa sebagian besar algoritma pembelajaran mesin, dan kNN adalah salah satu algoritma yang paling rentan terhadap tingginya dimensionalitas. Hal ini karena semakin banyak dimensi, ruang yang bisa ditempati instance semakin besar, sehingga semakin besar pula kemungkinan bahwa nearest neighbour dari suatu instance sebetulnya sama sekali tidak \u201cnear\u201c. Rentan terhadap perbedaan rentang variabel \u200b Dalam perhitungan jarak antar-instance, kNN menganggap semua variabel setara atau sama penting (lihat bagian penjumlahan pada rumus perhitungan jarak di atas). Jika terdapat variabel p yang memiliki rentang jauh lebih besar dibanding variabel-variabel lainnya, maka perhitungan jarak akan didominasi oleh p. Misalkan ada dua variabel, a dan b, dengan rentang variabel a 0 sampai 1.000 dan rentang variabel b 0 sampai 10. Kuadrat selisih dua nilai variabel b tidak akan lebih dari 100, sedangkan untuk variabel a kuadrat selisihnya bisa mencapai 1.000.000. Hal ini bisa mengecoh kNN sehingga kNN menganggap a tidak membawa pengaruh dalam perhitungan jarak karena rentangnya sangat besar dibanding rentang b.Ilustrasinya diberikan di bawah ini. Manakah yang merupakan nearest neighbour dari instance x? Jika dilihat dari \u201ckacamata\u201d komputer, nearest neighbour x bukanlah y, melainkan z, Mengapa? Untuk mengatasi perbedaan rentang, biasanya dilakukan preproses berupa standardisasi rentang semua variabel sebelum menerapkan algoritma kNN Nilai komputasi yang tinggi. \u200b Untuk mengklasifikasi sebuah instance x, kNN harus menghitung jarak antara x dengan semua instance lain dalam dataset yang kita miliki. Dengan kata lain, kompleksitas waktu klasifikasi kNN berbanding lurus dengan jumlah instance latih. Jika dataset yang kita miliki berukuran besar (terdiri dari banyak instance dan/atau banyak variabel), proses ini bisa jadi sangat lambat. Bayangkan, jika kita punya 10.000 instance dengan masing-masing 20 variabel dan kita ingin mengklasifikasi 100 instance baru (instance uji), maka total operasi yang harus dilakukan menjadi: (100 instance uji x 10.000 instance latih) x 20 variabel/instance x 2 operasi/variabel = 40 juta operasi Beberapa cara pengindexan (K-D tree) dapat digunakan untuk mereduksi biaya komputasi.","title":"Kekurangan"},{"location":"knn/#d-implementasi-haberman-dataset-dengan-knn-menggunakan-python-scikit-learn","text":"","title":"D. IMPLEMENTASI HABERMAN DATASET DENGAN KNN MENGGUNAKAN PYTHON SCIKIT LEARN"},{"location":"knn/#dataset","text":"Judul: Haberman's Survival Data Sources: (a) Donor: Tjen-Sien Lim ( limt@stat.wisc.edu ) (b) Date: March 4, 1999 Jumlah Instances: 306 Jumlah Attributes: 4 (termasuk attribute class) Informasi Attribute: Age of patient at time of operation (numerical) Patient's year of operation (year - 1900, numerical) Number of positive axillary nodes detected (numerical) Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year","title":"Dataset"},{"location":"knn/#implementasi","text":"","title":"Implementasi"},{"location":"knn/#mengimport-librari","text":"Install terlebih dahulu librari-librari yang dibutuhkan. maka, module dapat di eksekusi import numpy as np import matplotlib.pyplot as plt import pandas as pd","title":"Mengimport Librari"},{"location":"knn/#langkah-1-mengimport-dataset","text":"Untuk mengimport dan load data, anda dapat menggunkaan sintax dibawah ini: # memasukkan url dataset kedalam variabel, sehingga anda tidak perlu mendownload data. cukup memanggilnya url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\" # memasukkan nama kolom ke dalam dataset names = [ 'age' , 'patient-year-operation' , 'nodes-detected' , '(Survival-status)Class' ] # membaca dataset ke pandas dataframe dataset = pd . read_csv ( url , names = names ) Untuk melihat data, eksekusi code berikut: print ( dataset ) # atau dataset.head() Maka, akan menampilkan data seperti dibawah ini (row paling atas dan bawah):","title":"Langkah 1: Mengimport DataSet"},{"location":"knn/#langkah-2-preprocessing","text":"Langkah selanjutnya adalah memisahkan dataset kedalam atribut dan label. gunakan code berikut ini: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 3 ] . values variabel X untuk mendefinisikan 3 Kolom pada dataset sedangkan variabel y untuk mendefinisikan label pada dataset.","title":"Langkah 2: Preprocessing"},{"location":"knn/#langkah-3-train-test-split","text":"Untuk memisahkan data menjadi data training dan data testing. performa algoritma yang saya gunakan selama fase testing tidak terlihat (un-seen data). berikut code untuk membuat training dan testing data: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) script diatas memisahkan dataset 80% data training dan 20% data testing. dari 306 data, terbagi data training sebesar 245 data dan data testing sebesar 61 data.","title":"Langkah 3: Train Test Split"},{"location":"knn/#fitur-scaling","text":"Sebelum kita memprediksi data yang sebenarnya. alangkah baiknya kita menggunakan fitur scaling untuk menormalisasikan data sebelum di testing. berikut codenya: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test )","title":"Fitur Scaling"},{"location":"knn/#langkah-4-training-dan-prediction","text":"from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 7 ) classifier . fit ( X_train , y_train ) langkah pertama adlah mengimport librari KNeighborsClassifier class from the sklearn.neighbors . Pada baris kedua, class di inisialisasi dengan parameter, i.e. n_neigbours . nilai K yang diuji adalah K-7. Kemudian kita memprediksi nilai dari datset: y_pred = classifier . predict ( X_test )","title":"Langkah 4: Training dan Prediction"},{"location":"knn/#langkah-5-evaluating-dari-algoritma","text":"Untuk mengevaluasi sebuah algoritma, matrix, presisi, recall dan f1 score pada umumnya menggunnakan matrix dengan confusion_matrix dan classification_report methods dari sklearn.metrics Berikut code programnya: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) Tampilan output dari script diatas: dari hasil output diatas dapat diketahui bahwa nilai K-7 menghasilkan 71%","title":"Langkah 5: Evaluating dari Algoritma"},{"location":"knn/#langkah-6-mengekstrak-rating-error-pada-k-value","text":"Pada testing dan evaluasi data sebelumnya. kita telah mengecek akurasi pada K-. Namun, untuk menghasilkan nilai tingkat akurasi yang tinggi kita dapat mengecek beberapa sebagai acuan tanpa mengulang-ngulang data program yaitu dengan cara plot grafik dan coresponden nilai errror pada K. Untuk menghitung, nilai value saya menggunakan distance K11:40. anda dapat menggunakan script berikut: error = [] # menghitung nilai error pada K antara 1 dan 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) Script diatas mengeksekusi perulangan 1 sampai 40. pada setiap iterasi, prediksi nilai error yang telah dihitung akan masuk dalam list error. langkah selanjutnya adalah plot nilai error K. berikut script code: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = 'red' , linestyle = 'dashed' , marker = 'o' , markerfacecolor = 'blue' , markersize = 10 ) plt . title ( 'Rating Error Nilai K' ) plt . xlabel ( 'Nilai K' ) plt . ylabel ( 'Mean Error' ) Output","title":"Langkah 6: Mengekstrak rating error pada K Value"},{"location":"knn/#referensi","text":"https://id.wikipedia.org/wiki/KNN https://en.wikipedia.org/wiki/Feature_scaling https://belajarkalkulus.com/clustering-part-iii/ http://depandienda.it.student.pens.ac.id/file/knn_references.pdf https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/","title":"REFERENSI"}]}