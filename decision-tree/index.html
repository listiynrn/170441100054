



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Duties of Information System">
      
      
        <link rel="canonical" href="https://listiynrn.github.io/170441100054/decision-tree/">
      
      
        <meta name="author" content="Listiyo Nuraini">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>DECISION TREE CLASSIFIER - INFORMATION SYSTEM</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#a-penjelasan-decision-tree" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://listiynrn.github.io/170441100054/" title="INFORMATION SYSTEM" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              INFORMATION SYSTEM
            </span>
            <span class="md-header-nav__topic">
              DECISION TREE CLASSIFIER
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/listiynrn/170441100054" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    listiynrn/170441100054
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="DATA MINING" class="md-tabs__link md-tabs__link--active">
        DATA MINING
      </a>
    
  </li>

      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://listiynrn.github.io/170441100054/" title="INFORMATION SYSTEM" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    DATA MINING
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/listiynrn/170441100054" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    listiynrn/170441100054
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="PENGANTAR" class="md-nav__link">
      PENGANTAR
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../knn/" title="K-NEAREST NEIGHBOR CLASSIFIER" class="md-nav__link">
      K-NEAREST NEIGHBOR CLASSIFIER
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        DECISION TREE CLASSIFIER
      </label>
    
    <a href="./" title="DECISION TREE CLASSIFIER" class="md-nav__link md-nav__link--active">
      DECISION TREE CLASSIFIER
    </a>
    
      
<nav class="md-nav md-nav--secondary">
 
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#PENJELASAN-DECISION-TREE" title="A. PENJELASAN DECISION TREE" class="md-nav__link">
   A. PENJELASAN DECISION TREE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ALGORITMA-ID3" title="B. ALGORITMA ID3" class="md-nav__link">
    B. ALGORITMA ID3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ARSITEKTUR-DECISION-TREE" title="C. ARSITEKTUR DECISION TREE" class="md-nav__link">
    C. ARSITEKTUR DECISION TREE
  </a>
  
</li>
      
      
        <li class="md-nav__item">
  <a href="#PERHITUNGAN-SEDERHANA-DECISION-TREE" title="D. PERHITUNGAN SEDERHANA DECISION TREE" class="md-nav__link">
    D. PERHITUNGAN SEDERHANA DECISION TREE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#KELEBIHAN-DAN-KEKURANGAN-DECISION-TREE" title="E. KELEBIHAN DAN KEKURANGAN DECISION TREE" class="md-nav__link">
    E. KELEBIHAN DAN KEKURANGAN DECISION TREE
  
</a>
</li>
      
        <li class="md-nav__item">
  <a href="#IMPLEMENTASI-DECISION-TREE-HABERMAN-DATASET-MENGGUNAKAN-PYTHON-SCIKIT-LEARN" title="F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN" class="md-nav__link">
    F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN
  </a>
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
            
        <li class="md-nav__item">
  <a href="#PENJELASAN-DECISION-TREE" title="A. PENJELASAN DECISION TREE" class="md-nav__link">
   A. PENJELASAN DECISION TREE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ALGORITMA-ID3" title="B. ALGORITMA ID3" class="md-nav__link">
    B. ALGORITMA ID3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ARSITEKTUR-DECISION-TREE" title="C. ARSITEKTUR DECISION TREE" class="md-nav__link">
    C. ARSITEKTUR DECISION TREE
  </a>
  
</li>
      
       
      
        <li class="md-nav__item">
  <a href="#PERHITUNGAN-SEDERHANA-DECISION-TREE" title="D. PERHITUNGAN SEDERHANA DECISION TREE" class="md-nav__link">
    D. PERHITUNGAN SEDERHANA DECISION TREE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#KELEBIHAN-DAN-KEKURANGAN-DECISION-TREE" title="E. KELEBIHAN DAN KEKURANGAN DECISION TREE" class="md-nav__link">
    E. KELEBIHAN DAN KEKURANGAN DECISION TREE
  
</a>
</li>
      
        <li class="md-nav__item">
  <a href="#IMPLEMENTASI-DECISION-TREE-HABERMAN-DATASET-MENGGUNAKAN-PYTHON-SCIKIT-LEARN" title="F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN" class="md-nav__link">
    F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN
  </a>
</li>     
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="a-penjelasan-decision-tree"><strong>A. PENJELASAN DECISION TREE</strong><a class="headerlink" href="#a-penjelasan-decision-tree" title="Permanent link">&para;</a></h1>
<hr />
<p>Decision tree atau pohon keputusan adalah alat pendukung keputusan yang menggunakan model keputusan yang berbentuk seperti pohon. Decision tree memetakan berbagai alternatif yang mungkin untuk mengatasi suatu masalah, dan terdapat juga faktor-faktor kemungkinan yang dapat mempengaruhi alternatif tersebut beserta estimasi akhirnya jika memilih alternatif yang ada. Decision tree merupakan salah satu metode yang bisa digunaan untuk menampilkan algoritma dimana hanya berisi pernyataan kontrol bersyarat.</p>
<p>Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas–kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu</p>
<h2 id="decision-tree-terdiri-dari-tiga-jenis-simpul"><strong>Decision tree terdiri dari tiga jenis simpul:</strong><a class="headerlink" href="#decision-tree-terdiri-dari-tiga-jenis-simpul" title="Permanent link">&para;</a></h2>
<ul>
<li>Simpul keputusan - biasanya diwakili oleh kotak</li>
<li>Simpul peluang - biasanya diwakili oleh lingkaran</li>
<li>Simpul akhir - biasanya diwakili oleh segitiga</li>
</ul>
<h2 id="konsep-decision-tree"><strong>Konsep Decision Tree</strong><a class="headerlink" href="#konsep-decision-tree" title="Permanent link">&para;</a></h2>
<p>Decision tree digunakan untuk mengklasifikasikan suatu sampel data yang belum diketahui kelasnya ke dalam kelas yang sudah ada. Jalur pengujian data adalah pertama melalui root node dan terakhir adalah melalui leaf node yang akan menyimpulkan prediksi kelas bagi data tersebut. Atribut data harus berupa data kategorik, bila kontinu maka atribut harus didiskretisasi terlebih dahulu. </p>
<h2 id="karakteristik-decision-tree"><strong>Karakteristik Decision Tree</strong><a class="headerlink" href="#karakteristik-decision-tree" title="Permanent link">&para;</a></h2>
<p>Berikut ini adalah beberapa karakteristik decision tree secara umum : </p>
<p>• Decision tree merupakan suatu pendekatan nonparametrik untuk membangun model klasifikasi </p>
<p>• Teknik yang dikembangkan dalam membangun decision tree memungkinkan untuk membangun model secara cepat dari training set yang berukuran besar. </p>
<p>• Decision tree dengan ukuran tree yang kecil relatif mudah untuk menginterpretasinya. </p>
<p>• Decision tree memberikan gambaran yang ekpresif dalam pembelajaran fungsi nilai diskret. </p>
<p>• Algoritma decision tree cukup robbust terhadap munculnya noise terutama untuk metode yang dapat menangani masalah overfitting. </p>
<p>• Adanya atribut yang berlebihan tidak terlalu mengurangi akurasi decision tree . </p>
<p>• Karena sebagian algoritma decision tree menggunakan pendekatan topdown, yaitu partisi dilakukan secara rekursif maka jumlah record menjadi lebih kecil. Pada leaf node, jumlah record mungkin akan terlalu kecil untuk dapat membuat keputusan secara statistik tentang representasi kelas dari suatu node. </p>
<p>• Sebuah subtree dapat direplikasi beberapa kali dalam decision tree tetapi ini akan menyebabkan decision tree menjadi lebih kompleks dan lebih sulit untuk diinterpretasi. </p>
<h1 id="b-algoritma-id3"><strong>B. ALGORITMA ID3</strong><a class="headerlink" href="#b-algoritma-id3" title="Permanent link">&para;</a></h1>
<hr />
<p>Algoritma ID3 merupakan algoritma yang dipergunakan untuk membangun sebuah decision tree atau pohon keputusan. Algoritma ini ditemukan oleh J. Ross Quinlan (1979), dengan memanfaatkan Teori Informasi atau Information Theory milik Shanon. ID3 sendiri merupakan singkatan dari Iterative Dichotomiser 3.</p>
<p>Decision tree menggunakan struktur hierarki untuk pembelajaran supervised. Proses dari decision tree dimulai dari root node hingga leaf node yang dilakukan secara rekursif. Di mana setiap percabangan menyatakan suatu kondisi yang harus dipenuhi dan pada setiap ujung pohon menyatakan kelas dari suatu data.</p>
<p>Proses dalam decision tree yaitu mengubah bentuk data (tabel) menjadi model pohon (tree) kemudian mengubah model pohon tersebut menjadi aturan (rule).</p>
<h1 id="c-arsitektur-decision-tree"><strong>C. ARSITEKTUR DECISION TREE</strong><a class="headerlink" href="#c-arsitektur-decision-tree" title="Permanent link">&para;</a></h1>
<hr />
<p>Arsitektur pohon keputusan dibuat menyerupai bentuk pohon, dimana pada umumnya sebuah pohon terdapat akar (root), cabang dan daun (leaf). Pada pohon keputusan juga terdiri  dari tiga bagian sebagai berikut :</p>
<p>a. <strong>Root node</strong> atau node akar merupakan node yang terletak paling atas dari suatu pohon.</p>
<p>b. <strong>Internal Node</strong> ini merupakan node percabangan, dimana pada node ini hanya terdapat satu input dan mempunyai minimal dua output.</p>
<p>c. <strong>Leaf Node</strong> ini merupakan node akhir, hanya memiliki satu input, dan tidak memiliki output. Pada pohon keputusan setiap leaf node menandai label kelas.</p>
<p>Pada pohon keputusan di setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan nilai kelas data. Gambar berikut merupakan bentuk arsitektur pohon keputusan.</p>
<p>Pemilihan atribut untuk menjadi rootnode atau internal node sebagai atribut test berdasarkan atas ukuran impurity dari masing–masing atribut. Ukuran–ukuran impurity yang umumnya digunakan adalah information gain, gain ratio dan gini index. Atribut yang memiliki nilai impurity tertinggi akan dipilih sebagai atribut test. </p>
<p><img alt="Arsitecture Decision Tree" src="../assets/images/arsitekturdt.jpg" /></p>
<h2 id="langkah-langkah-konstruksi-pohon-keputusan-dengan-algoritma-id3"><strong>Langkah-Langkah Konstruksi Pohon Keputusan dengan Algoritma ID3</strong><a class="headerlink" href="#langkah-langkah-konstruksi-pohon-keputusan-dengan-algoritma-id3" title="Permanent link">&para;</a></h2>
<p>Adapun langkah-langkah dalam konstruksi pohon keputusan adalah sebagai berikut :</p>
<p><strong>Langkah 1 :</strong> Pohon dimulai dengan sebuah simpul yang mereperesentasikan sampel data pelatihan yaitu dengan membuat simpul akar.</p>
<p><strong>Langkah 2 :</strong> Jika semua sampel berada dalam kelas yang sama, maka simpul ini menjadi daun dan dilabeli menjadi kelas. Jika tidak, information gain akan digunakan untuk memilih atribut terbaik dalam memisahkan data sampel menjadi kelas-kelas individu.</p>
<p><strong>Langkah 3 :</strong> Cabang akan dibuat untuk setiap nilai pada atribut dan data sampel akan dipartisi lagi.</p>
<p><strong>Langkah 4 :</strong> Algoritma ini menggunakan proses rekursif untuk membentuk pohon keputusan pada setiap data partisi. Jika sebuah atribut sduah digunakan disebuah simpul, maka atribut ini tidak akan digunakan lagi di simpul anak-anaknya.</p>
<p><strong>Langkah 5 :</strong> Proses ini berhenti jika dicapai kondisi seperti berikut :</p>
<p>– Semua sampel pada simpul berada di dalam satu kelas</p>
<p>– Tidak ada atribut lainnya yang dapat digunakan untuk mempartisi sampel lebih lanjut. Dalam hal ini akan diterapkan suara terbanyak. Ini berarti mengubah sebuah simpul menjadi daun dan melabelinya dnegan kelas pada suara terbanyak.</p>
<h2 id="entropy-information-gain"><strong>Entropy &amp; Information Gain</strong><a class="headerlink" href="#entropy-information-gain" title="Permanent link">&para;</a></h2>
<p>Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur “seberapa informatifnya” sebuah node (yang biasanya disebut seberapa baiknya).</p>
<p>Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama.</p>
<p>Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama.</p>
<p>0 &lt; Entropi(S) &lt; 1, jika jumlah contoh positif dan negatif dalam S tidak sama.
$$
\begin{equation}
(S)=\sum_{j=1}^{k}-p_{j} \log <em>{2} p</em>{j}
\end{equation}
$$
Dimana:</p>
<p>• <em>S</em> adalah himpunan (dataset) kasus</p>
<p>• <em>k</em> adalah banyaknya partisi <em>S</em></p>
<p>• <em>pj</em> adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus.</p>
<p>Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar.
$$
\begin{equation}
\operatorname{Gain}(A)=E n t r o p i(S)-\sum_{i=1}^{k} \frac{\left|S_{i}\right|}{|S|} \times E n t r o p i\left(S_{i}\right)
\end{equation}
$$
Dimana:</p>
<p>S = ruang (data) sample yang digunakan untuk training.</p>
<p>A = atribut.</p>
<p>|Si| = jumlah sample untuk nilai V.</p>
<p>|S| = jumlah seluruh sample data.</p>
<p>Entropi(Si) = entropy untuk sample-sample yang memiliki nilai <em>i</em></p>
<h1 id="d-perhitungan-sederhana-decision-tree"><strong>D. PERHITUNGAN SEDERHANA DECISION TREE</strong><a class="headerlink" href="#d-perhitungan-sederhana-decision-tree" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="contoh"><strong><em>Contoh</em></strong><a class="headerlink" href="#contoh" title="Permanent link">&para;</a></h2>
<p>Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti <strong>Cuaca</strong>, <strong>Suhu</strong>, <strong>Kelembaban</strong>, dan <strong>Berangin</strong>. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas “<strong>Tidak</strong>” dan kelas “<strong>Ya</strong>”. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 “Ya” dan 4 “Tidak” pada kolom Main.</p>
<table>
<thead>
<tr>
<th align="center">NO</th>
<th align="center">CUACA</th>
<th align="center">SUHU</th>
<th align="center">KELEMBAPAN</th>
<th align="center">BERANGIN</th>
<th align="center">MAIN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1</td>
<td align="center">cerah</td>
<td align="center">panas</td>
<td align="center">tinggi</td>
<td align="center">salah</td>
<td align="center">tidak</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">cerah</td>
<td align="center">panas</td>
<td align="center">tinggi</td>
<td align="center">benar</td>
<td align="center">tidak</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">berawan</td>
<td align="center">panas</td>
<td align="center">tinggi</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">hujan</td>
<td align="center">sejuk</td>
<td align="center">tinggi</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">hujan</td>
<td align="center">dingin</td>
<td align="center">normal</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">hujan</td>
<td align="center">dingin</td>
<td align="center">normal</td>
<td align="center">benar</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">berawan</td>
<td align="center">dingin</td>
<td align="center">normal</td>
<td align="center">benar</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">cerah</td>
<td align="center">sejuk</td>
<td align="center">tinggi</td>
<td align="center">salah</td>
<td align="center">tidak</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">cerah</td>
<td align="center">dingin</td>
<td align="center">normal</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">hujan</td>
<td align="center">sejuk</td>
<td align="center">normal</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">11</td>
<td align="center">cerah</td>
<td align="center">sejuk</td>
<td align="center">normal</td>
<td align="center">benar</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">12</td>
<td align="center">berawan</td>
<td align="center">sejuk</td>
<td align="center">tinggi</td>
<td align="center">benar</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">13</td>
<td align="center">berawan</td>
<td align="center">panas</td>
<td align="center">normal</td>
<td align="center">salah</td>
<td align="center">ya</td>
</tr>
<tr>
<td align="center">14</td>
<td align="center">hujan</td>
<td align="center">sejuk</td>
<td align="center">normal</td>
<td align="center">benar</td>
<td align="center">tidak</td>
</tr>
</tbody>
</table>
<h2 id="penyelesaian"><strong><em>Penyelesaian</em></strong><a class="headerlink" href="#penyelesaian" title="Permanent link">&para;</a></h2>
<p>hitung entropi</p>
<p>*Entropi (S) = (-(10/14) x log*2 *(10/14) + (-(4/10) x log*2 <em>(4/10)) =</em> <strong>0.863120569</strong></p>
<table>
<thead>
<tr>
<th align="center">Total Kasus</th>
<th align="center">sum(YA)</th>
<th align="center">sum(TIDAK)</th>
<th align="center">Entropi Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">14</td>
<td align="center">10</td>
<td align="center">4</td>
<td align="center">0.863120569</td>
</tr>
</tbody>
</table>
<p>Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya.</p>
<p><img alt="Aanalisis atribut, nilai, banyaknya kejadian, entropi dan gain" src="../assets/images/contoh2.1.jpg" /></p>
<p>Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel.</p>
<p><em>Gain (Cuaca)</em> = 0.863120569 –  ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = <strong>0.258521037</strong></p>
<p>Hitung pula Gain (Suhu), Gain (Kelembaban), dan Gain (Berangin). <strong>Karena nilai gain terbesar adalah Gain (Kelembaban), maka atribut “Kelembaban” menjadi node akar (root node)</strong>.</p>
<p>Kemudian pada “Kelembaban” normal, memiliki 7 kasus dan semuanya memiliki jawaban Ya (Sum(Total) / Sum(Ya) = 7/7 = 1). Dengan demikian “Kelembaban” normal menjadi daun atau <em>leaf</em>.</p>
<p><img alt="decision tree node 1 (assets/images/leaf1.jpg)" src="../assets/images/leaf1.jpg" /></p>
<p>Berdasarkan pembentukan pohon keputusan node 1 (root node), Node 1.1 akan dianalisis lebih lanjut. Untuk mempermudah, Tabel dibawah difilter, dengan mengambil data yang memiliki “Kelembaban” = Tinggi.</p>
<p><img alt="atribut data kelembapan=tinggi" src="../assets/images/contoh2.2.jpg" /></p>
<p>Kemudian dihitung nilai entropi atribut “Kelembaban” Tinggi dan entropi setiap atribut serta gainnya. Setelah itu tentukan pilih atribut yang memiliki gain tertinggi untuk dibuatkan node berikutnya.</p>
<p><img alt="hasil analisis node 1.1" src="../assets/images/contoh2.3.jpg" /></p>
<p>Gain tertinggi yang didapat ada pada atribut “Cuaca”, dan Nilai yang dijadikan daun atau <em>leaf</em> adalah Berawan dan Cerah. Jika divualisasi maka pohon keputusan tampak seperti Gambar dibawah.</p>
<p>Untuk menganalisis node 1.1.2, lakukan lagi langkah-langkah yang sama seperti sebelumnya hingga semua node beberntuk node <em>leaf</em>.</p>
<p><img alt="decision tree analisi nde 1.1." src="../assets/images/contoh2.4.jpg" /></p>
<p><img alt="hasil analisis node 1.1.2" src="../assets/images/contoh2.5.jpg" /></p>
<p><img alt="decision tree akhir" src="../assets/images/contoh2.6.jpg" /></p>
<h1 id="e-kelebihan-dan-kekurangan-decision-tree"><strong>E. KELEBIHAN DAN KEKURANGAN DECISION TREE</strong><a class="headerlink" href="#e-kelebihan-dan-kekurangan-decision-tree" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="kelebihan"><strong>KELEBIHAN</strong><a class="headerlink" href="#kelebihan" title="Permanent link">&para;</a></h2>
<ol>
<li>Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi simple dan spesifik.</li>
<li>Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka contoh diuji hanya berdasarkan kriteria atau kelas-kelas tertentu.</li>
<li>Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama.</li>
<li>Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan kriteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.</li>
</ol>
<h2 id="kekurangan"><strong>KEKURANGAN</strong><a class="headerlink" href="#kekurangan" title="Permanent link">&para;</a></h2>
<ol>
<li>Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan.</li>
<li>Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar.</li>
<li>Kesulitan dalam mendesain pohon keputusan yang optimal</li>
<li>Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.</li>
</ol>
<h1 id="f-implementasi-decision-tree-haberman-dataset-menggunakan-python-scikit-learn"><strong>F. IMPLEMENTASI DECISION TREE HABERMAN DATASET MENGGUNAKAN PYTHON SCIKIT LEARN</strong><a class="headerlink" href="#f-implementasi-decision-tree-haberman-dataset-menggunakan-python-scikit-learn" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="dataset"><strong>Dataset</strong><a class="headerlink" href="#dataset" title="Permanent link">&para;</a></h2>
<ol>
<li>Judul: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data">Haberman's Survival Data</a></li>
<li>Sources:
   (a) Donor:   Tjen-Sien Lim (<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#105;&#109;&#116;&#64;&#115;&#116;&#97;&#116;&#46;&#119;&#105;&#115;&#99;&#46;&#101;&#100;&#117;">&#108;&#105;&#109;&#116;&#64;&#115;&#116;&#97;&#116;&#46;&#119;&#105;&#115;&#99;&#46;&#101;&#100;&#117;</a>)
   (b) Date:    March 4, 1999</li>
<li>Jumlah Instances: 306</li>
<li>Jumlah Attributes: 4 (termasuk attribute class)</li>
<li>Informasi Attribute:</li>
<li>Age of patient at time of operation (numerical)</li>
<li>Patient's year of operation (year - 1900, numerical)</li>
<li>Number of positive axillary nodes detected (numerical)</li>
<li>Survival status (class attribute)
      1 = the patient survived 5 years or longer
      2 = the patient died within 5 year</li>
</ol>
<h2 id="implementasi"><strong>Implementasi</strong><a class="headerlink" href="#implementasi" title="Permanent link">&para;</a></h2>
<h3 id="langkah-1-import-librari-python-machine-learning"><strong>Langkah 1: Import Librari Python Machine Learning</strong><a class="headerlink" href="#langkah-1-import-librari-python-machine-learning" title="Permanent link">&para;</a></h3>
<p>Bagian ini melibatkan mengimpor semua perpustakaan yang akan kita gunakan. Kami mengimpor modul numpy dan sklearn train_test_split, DecisionTreeClassifier &amp; akurasi_score.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> 
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
</pre></div>

<p>Numpy array dan panda dataframe akan membantu kita dalam memanipulasi data. Seperti dibahas di atas, sklearn adalah perpustakaan pembelajaran mesin. Metode cross_validation train_test_split () akan membantu dengan memecah data menjadi train &amp; test set.
Modul tree akan digunakan untuk membangun Decision Tree Classifier. Modul Accutacy_score akan digunakan untuk menghitung metrik akurasi dari variabel kelas yang diprediksi.</p>
<h3 id="langkah-2-data-import"><strong>Langkah 2: Data Import</strong><a class="headerlink" href="#langkah-2-data-import" title="Permanent link">&para;</a></h3>
<p>Untuk mengimpor data dan memanipulasinya, kita akan menggunakan kerangka data panda. Pertama-tama, kita perlu mengunduh dataset. Anda dapat mengunduh dataset dari sini. Semua nilai data dipisahkan oleh koma.
Setelah mengunduh file data, kami akan menggunakan metode Pandas read_csv () untuk mengimpor data ke dalam kerangka data panda. Karena data kami dipisahkan dengan koma "," dan tidak ada tajuk dalam data kami, jadi kami akan menempatkan nilai parameter  "None" dan nilai parameter sep sebagai ",".</p>
<div class="codehilite"><pre><span></span><span class="n">balance_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
<span class="s1">&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data&#39;</span><span class="p">,</span>
                           <span class="n">sep</span><span class="o">=</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>

<p>Kami menyimpan data ke dalam variabel data "balance_data".</p>
<p>Untuk memeriksa panjang &amp; dimensi kerangka data, kami dapat menggunakan metode len () &amp; “.shape”.</p>
<div class="codehilite"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Dataset Lenght:: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">balance_data</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Dataset Shape:: &quot;</span><span class="p">,</span> <span class="n">balance_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><img alt="" src="../assets/images/open2.1.PNG" /></p>
<p>Kita dapat print head .e, menggunakan method <strong>head()</strong> atau method <strong>print</strong> itu sendiri</p>
<div class="codehilite"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Dataset:: &quot;</span><span class="p">)</span>
<span class="n">balance_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> <span class="c1">#atau print (balance_data)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><img alt="row data teratas" src="../assets/images/open2.2.PNG" /></p>
<p><img alt="row data terbawah" src="../assets/images/open2.3.PNG" /></p>
<h3 id="langkah-3-data-slicing"><strong>Langkah 3: Data Slicing</strong><a class="headerlink" href="#langkah-3-data-slicing" title="Permanent link">&para;</a></h3>
<p>Data Slicing (Mengiris data) adalah langkah untuk membagi data menjadi set training dan testing. Kumpulan data pelatihan dapat digunakan secara khusus untuk pembangunan model. Dataset testing tidak boleh dicampuradukkan saat membangun model. Bahkan selama standardisasi, kita tidak boleh menstandarisasi set testing.</p>
<div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">balance_data</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">balance_data</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

<p>script di atas membagi data menjadi set fitur &amp; target yang ditetapkan. Set "X" terdiri dari variabel prediktor. Ini terdiri dari data dari kolom 2 hingga kolom 5. Set "Y" terdiri dari variabel hasil. Ini terdiri dari data di kolom 1. Kami menggunakan ".values" dari numpy yang mengubah dataframe kami menjadi array numpy.</p>
<p>Mari kita pisahkan data kita menjadi set training dan testing. Kami akan menggunakan metode sklearn train_test_split ().</p>
<div class="codehilite"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>

<p>script di atas akan membagi data menjadi set training dan testing. X_train, y_train adalah <strong>data training</strong> &amp; X_test, y_test milik <strong>dataset testing</strong>.</p>
<p>Parameter test_size diberi nilai 0,3; itu berarti set tes akan menjadi 30% dari seluruh dataset &amp; ukuran dataset training akan menjadi 70% dari seluruh dataset. variabel random_state adalah keadaan generator angka pseudo-acak yang digunakan untuk pengambilan sampel acak. Jika Anda ingin mereplikasi hasil kami, maka gunakan nilai random_state yang sama.</p>
<h3 id="langkah-4-decision-tree-training"><strong>Langkah 4: Decision Tree Training</strong><a class="headerlink" href="#langkah-4-decision-tree-training" title="Permanent link">&para;</a></h3>
<p>Sekarang kita mencocokan algoritma Decision Tree pada data training, memprediksi label untuk dataset validasi dan mencetak akurasi model menggunakan berbagai parameter.</p>
<p><strong>DecisionTreeClassifier ():</strong> Ini adalah fungsi classifier untuk DecisionTree merupakan fungsi utama untuk mengimplementasikan algoritma. Beberapa parameter penting adalah:</p>
<ul>
<li><strong>Criterion</strong>: Ini mendefinisikan fungsi untuk mengukur kualitas split. Sklearn mendukung kriteria "gini" untuk Indeks Gini &amp; "entropi" untuk Penguatan Informasi. Secara default, dibutuhkan nilai "gini".</li>
<li><strong>splitter</strong>: Ini mendefinisikan strategi untuk memilih split pada setiap node. Mendukung nilai "best" untuk memilih split terbaik &amp; "random" untuk memilih split acak terbaik. Secara default, dibutuhkan nilai "best".</li>
<li><strong>max_features</strong>: Ini menentukan no. fitur yang perlu dipertimbangkan ketika mencari perpecahan terbaik. Kami dapat memasukkan nilai integer, float, string &amp; None.
  Jika integer dimasukkan, maka nilai tersebut dianggap sebagai fitur maksimal pada setiap pemisahan.
  Jika nilai float diambil maka ini menunjukkan persentase fitur di setiap split.
  Jika "otomatis" atau "sqrt" diambil maka max_features = sqrt (n_features).
  Jika "log2" diambil maka max_features = log2 (n_features).
  Jika None, maka max_features = n_features. Secara default, dibutuhkan nilai "none".</li>
<li><strong>max_depth</strong>: Parameter max_depth menunjukkan kedalaman maksimum pohon. Itu bisa mengambil nilai integer atau none. Jika none, maka node diperluas sampai semua daun murni atau sampai semua daun mengandung kurang dari sampel min_samples_split. Secara default, dibutuhkan nilai "None".</li>
<li><strong>min_samples_split</strong>: Ini memberitahu di atas no minimum. sampel reqd. untuk membagi simpul internal. Jika nilai integer diambil maka pertimbangkan min_samples_split sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai "2".</li>
<li><strong>min_samples_leaf</strong>: Jumlah sampel minimum yang diperlukan berada pada simpul daun. Jika nilai integer diambil maka pertimbangkan min_samples_leaf sebagai no minimum. Jika mengambang, maka itu menunjukkan persentase. Secara default, dibutuhkan nilai “1”.</li>
<li><strong>max_leaf_nodes</strong>: Ini mendefinisikan jumlah maksimum dari node leaf yang mungkin. Jika none maka dibutuhkan jumlah daun yang tidak terbatas. Secara default, dibutuhkan nilai "None".</li>
<li><strong>min_impurity_split</strong>: Ini mendefinisikan ambang batas untuk menghentikan pertumbuhan pohon awal. Suatu simpul akan terpecah jika kenajisannya berada di atas ambang batas kalau tidak itu adalah daun.</li>
</ul>
<p>Mari kita buat pengklasifikasi menggunakan kriteria sebagai indeks gini &amp; keuntungan informasi. Kita harus menyesuaikan classifier kita menggunakan fit (). Kami akan memplot visualisasi classifier pohon keputusan kami juga.</p>
<p><strong>Decision Tree Classifier dengan kriteria GINI</strong></p>
<div class="codehilite"><pre><span></span><span class="n">clf_gini</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;gini&quot;</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">306</span><span class="p">,</span>
                               <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf_gini</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">306</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><strong>Decision Tree Classifier dengan kriteria information gain</strong></p>
<div class="codehilite"><pre><span></span><span class="n">clf_entropy</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;entropy&quot;</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">306</span><span class="p">,</span>
 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">clf_entropy</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;entropy&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">306</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<h3 id="langkah-5-prediction"><strong>Langkah 5: Prediction</strong><a class="headerlink" href="#langkah-5-prediction" title="Permanent link">&para;</a></h3>
<p>Sekarang, kami telah memodelkan 2 pengklasifikasi. Satu classifier dengan indeks gini &amp; satu lagi dengan information gain sebagai kriteria. Kami siap memprediksi kelas untuk set testing kami. Kita dapat menggunakan metode predict (). Mari kita coba memprediksi variabel target untuk catatan pertama kelompok testing.</p>
<div class="codehilite"><pre><span></span><span class="n">clf_gini</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
</pre></div>

<p>Dengan cara ini kita dapat memprediksi kelas untuk satu catatan. Sudah waktunya untuk memprediksi variabel target untuk seluruh dataset pengujian.</p>
<h4 id="prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index"><strong>Prediction untuk Decision Tree classifier dengan criterion sebagai gini index</strong><a class="headerlink" href="#prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf_gini</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><img alt="" src="../assets/images/prediksigini.PNG" /></p>
<h4 id="prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-information-gain"><strong>Prediction untuk Decision Tree classifier dengan criterion sebagai information gain</strong><a class="headerlink" href="#prediction-untuk-decision-tree-classifier-dengan-criterion-sebagai-information-gain" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><span class="n">y_pred_en</span> <span class="o">=</span> <span class="n">clf_entropy</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_pred_en</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<h3 id="langkah-6-menghitung-score-akurasi"><strong>Langkah 6: Menghitung Score Akurasi</strong><a class="headerlink" href="#langkah-6-menghitung-score-akurasi" title="Permanent link">&para;</a></h3>
<p>Function akurasi_score () akan digunakan untuk mencetak akurasi algoritma Decision Tree. Secara akurat, maksud kami adalah rasio titik data yang diprediksi dengan benar dengan semua titik data yang diprediksi. Akurasi sebagai matrik membantu memahami efektivitas algoritme . Dibutuhkan 4 parameter.</p>
<ul>
<li>y_true,</li>
<li>y_pred, </li>
<li>Normalize,</li>
<li>sample_weight.</li>
</ul>
<p>Dari 4 ini, normalisasi &amp; sample_weight adalah parameter opsional. Parameter y_true menerima larik label yang benar dan y_pred mengambil larik label yang diprediksi yang dikembalikan oleh classifier. Ini mengembalikan akurasi sebagai nilai float.</p>
<h4 id="accuracy-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index"><strong>Accuracy untuk Decision Tree classifier dengan criterion sebagai gini index</strong><a class="headerlink" href="#accuracy-untuk-decision-tree-classifier-dengan-criterion-sebagai-gini-index" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy gini index &quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><img alt="" src="../assets/images/akurasigini.PNG" /></p>
<h4 id="accuracy-untuk-decision-tree-classifier-denga-criterion-sebagai-information-gain"><strong>Accuracy untuk Decision Tree classifier denga  criterion sebagai information gain</strong><a class="headerlink" href="#accuracy-untuk-decision-tree-classifier-denga-criterion-sebagai-information-gain" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy information gain &quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred_en</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</pre></div>

<p><strong>Output</strong></p>
<p><img alt="" src="../assets/images/akurasigain.PNG" /></p>
<h1 id="referensi"><strong>REFERENSI</strong><a class="headerlink" href="#referensi" title="Permanent link">&para;</a></h1>
<p><a href="https://informatikalogi.com/algoritma-id3/">https://informatikalogi.com/algoritma-id3/</a></p>
<p><a href="https://sinta.unud.ac.id/uploads/wisuda/1108605014-3-Bab%20II.pdf">https://sinta.unud.ac.id/uploads/wisuda/1108605014-3-Bab%20II.pdf</a></p>
<p><a href="https://www.academia.edu/35688101/MAKALAH_KLASIFIKASI_DECISION_TREE">https://www.academia.edu/35688101/MAKALAH_KLASIFIKASI_DECISION_TREE</a></p>
<p><a href="https://garudacyber.co.id/artikel/1545-pengertian-dan-penerapan-decision-tree">https://garudacyber.co.id/artikel/1545-pengertian-dan-penerapan-decision-tree</a></p>
<p><a href="http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/">http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/</a></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../knn/" title="K-NEAREST NEIGHBOR CLASSIFIER" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                K-NEAREST NEIGHBOR CLASSIFIER
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 Listiyo Nuraini
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://linktree.com/spirulinaherb.co" class="md-footer-social__link fa fa-global"></a>
    
      <a href="https://github.com/listiynrn" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/listiyo_nuraini" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="http://instagram.com/listiy.nrn" class="md-footer-social__link fa fa-instagram"></a>
    
      <a href="https://linkedin.com/in/listiyo_nuraini" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>